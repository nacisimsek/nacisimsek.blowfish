[{"content":"Airflow Introduction Pipeline # In this article, we will be deploying Apache Airflow, and create a sample pipeline which fetches data from a webserver and write into MinIO bucket.\n","date":"2024-06-09","permalink":"/posts/20240609-airflow-nginx-minio/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Airflow Introduction Pipeline"},{"content":"","date":"2024-06-09","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"2024-06-09","permalink":"/tags/catalog/","section":"Tags","summary":"","title":"Catalog"},{"content":"","date":"2024-06-09","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"Change Data Capture (CDC) Pipeline Implementation # In this article, we will implement a pipeline with PostgreSQL, Debezium CDC, Kafka, MinIO and the Spark.\n","date":"2024-06-09","permalink":"/posts/20240609-debezium-cdc-flink/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Change Data Capture (CDC) Pipeline Implementation"},{"content":"","date":"2024-06-09","permalink":"/categories/data-engineering/","section":"Categories","summary":"","title":"Data Engineering"},{"content":"","date":"2024-06-09","permalink":"/categories/docker/","section":"Categories","summary":"","title":"Docker"},{"content":"Elasticsearch Indexing and Kibana Dashboard with PySpark # In this article, we will be sinking data to ElasticSearch by PySpark and create a dashboard on Kibana\n","date":"2024-06-09","permalink":"/posts/20240609-elasticsearch-kibana/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Elasticsearch Indexing and Kibana Dashboard with PySpark"},{"content":"","date":"2024-06-09","permalink":"/categories/hadoop/","section":"Categories","summary":"","title":"Hadoop"},{"content":"","date":"2024-06-09","permalink":"/tags/hdfs/","section":"Tags","summary":"","title":"Hdfs"},{"content":"","date":"2024-06-09","permalink":"/tags/hive/","section":"Tags","summary":"","title":"Hive"},{"content":"","date":"2024-06-09","permalink":"/tags/mapreduce/","section":"Tags","summary":"","title":"Mapreduce"},{"content":" Technical Account Manager @ Ververica ","date":"2024-06-09","permalink":"/authors/nacisimsek/","section":"Authors","summary":"","title":"Naci Simsek"},{"content":"Optimizing Spark Applications # In this article, we will be deploying Hive services on Hadoop cluster\n","date":"2024-06-09","permalink":"/posts/20240609-spark-optimization/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Optimizing Spark Applications"},{"content":"","date":"2024-06-09","permalink":"/tags/postgres/","section":"Tags","summary":"","title":"Postgres"},{"content":"Processing Complex Nested JSON File with Spark # In this article, we will be processing complex nested JSON file with Apache Spark\n","date":"2024-06-09","permalink":"/posts/20240609-spark-json-process/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Processing Complex Nested JSON File with Spark"},{"content":"Spark Streaming Hands On from/to Kafka # In this article, we will be developing a Spark Streaming application which will read data from Kafka, process, and write back to Kafka\n","date":"2024-06-09","permalink":"/posts/20240609-spark-streaming/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Spark Streaming Hands On from/to Kafka"},{"content":"","date":"2024-06-09","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"2024-06-09","permalink":"/tags/tutorial/","section":"Tags","summary":"","title":"Tutorial"},{"content":"Submitting Spark Application # In this article, we will be submitting Spark application to the Spark cluster we previously deployed\n","date":"2024-06-08","permalink":"/posts/20240608-spark-submit/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Submitting Spark Application"},{"content":"Spark DataFrame Operations # In this article, we will be practicing Spark DataFrame operations\n","date":"2024-06-07","permalink":"/posts/20240607-spark-dataframe/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Spark DataFrame Operations"},{"content":"Use PySpark for Data Clean up # In this article, we will be cleaning up a dirty data by using PySpark\n","date":"2024-06-06","permalink":"/posts/20240606-spark-cleanup-data/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Use PySpark for Data Clean up"},{"content":"Deploy Spark Cluster # In this article, we will be deploying Spark Cluster on local, docker env, and Kubernetes\n","date":"2024-06-05","permalink":"/posts/20240605-spark-deploy/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Deploy Spark Cluster"},{"content":"","date":"2024-06-05","permalink":"/categories/spark/","section":"Categories","summary":"","title":"Spark"},{"content":"","date":"2024-06-05","permalink":"/tags/spark/","section":"Tags","summary":"","title":"Spark"},{"content":"Kafka Python Operations # In this article, we will be deploying Hive services on Hadoop cluster\n","date":"2024-06-04","permalink":"/posts/20240604-kafka-python-operations/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Kafka Python Operations"},{"content":"","date":"2024-06-03","permalink":"/tags/configuration/","section":"Tags","summary":"","title":"Configuration"},{"content":"","date":"2024-06-03","permalink":"/categories/deployment/","section":"Categories","summary":"","title":"Deployment"},{"content":"","date":"2024-06-03","permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker"},{"content":"","date":"2024-06-03","permalink":"/categories/kafka/","section":"Categories","summary":"","title":"Kafka"},{"content":"","date":"2024-06-03","permalink":"/tags/kafka/","section":"Tags","summary":"","title":"Kafka"},{"content":"Kafka Topics and Operations # This article is about how to operate on Kafka topics, their management, and configure important parameters\n","date":"2024-06-03","permalink":"/posts/20240603-kafka-topics/","section":"Posts","summary":"This article is about how to operate on Kafka topics, their management, and configure important parameters","title":"Kafka Topics and Operations"},{"content":"","date":"2024-06-03","permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes"},{"content":"","date":"2024-06-03","permalink":"/tags/topics/","section":"Tags","summary":"","title":"Topics"},{"content":"","date":"2024-06-02","permalink":"/tags/brew/","section":"Tags","summary":"","title":"Brew"},{"content":"Deploy Kafka Cluster # In this article, we will be deploying Kafka Cluster on local, docker env, and Kubernetes\n","date":"2024-06-02","permalink":"/posts/20240602-kafka-deploy/","section":"Posts","summary":"This article is about how to deploy Kafka Cluster on local (MacOS), docker and kubernetes","title":"Deploy Kafka Cluster"},{"content":"","date":"2024-06-02","permalink":"/tags/helm/","section":"Tags","summary":"","title":"Helm"},{"content":"Hive Partitioning and Bucketing # In the previous article, we created Hive tables and observe data usage on HDFS and metadata management.\nIn this article, we will be performing partitioning and bucketing options and observe how applying these techniques can help us on query performance.\nInfo:\nIf you directly opened this article without setting up your Docker environment, I suggest you visit that article to deploy your cluster first.\nData Organization in Hive # There are four main components for organizing data in Hive: databases, tables, partitions, and buckets. Partitions and buckets are key techniques to improve query performance by dividing tables into smaller, manageable pieces.\nThink of it this way:\nDatabase: Corresponds to a directory in HDFS. Table: Also corresponds to a directory within the database directory in HDFS. Partition: A subdirectory within the table directory, used to further divide data based on partition keys. Bucket: File segments within a partition (or table if not partitioned), organizing data within those directories. Partitioning allows for faster data access by co-locating frequently queried data within the same folders. Bucketing, on the other hand, distributes data into file segments based on a hash of the bucket column, which is beneficial for query and join performance.\nImagine a large retail chain that processes thousands of sales orders every day. To manage this massive flow of data efficiently, the company organizes its orders in two steps:\n1. Partitioning by Date:\nEvery order is stored in a folder corresponding to the day it was placed. For instance, all orders from September 1, 2023, are stored together, all orders from September 2, 2023, are in another folder, and so on. This means that if you need to retrieve or analyze orders from a particular day, you only need to look at that day\u0026rsquo;s folder rather than the entire dataset.\n2. Bucketing by Store Location:\nWithin each daily folder, orders are further grouped by the store where they were made. Each store\u0026rsquo;s orders are kept in a separate bucket. So, if the chain has multiple stores (for example, Store A, Store B, Store C), orders from the same store on a specific day will be grouped together. This makes it much faster to run queries that focus on a particular store, such as joining order data with store-specific information.\nHere\u0026rsquo;s a simple tree representation to illustrate this:\nSales Orders Warehouse ├── Date: 2023-09-01 │ ├── Bucket: Store A │ ├── Bucket: Store B │ └── Bucket: Store C ├── Date: 2023-09-02 │ ├── Bucket: Store A │ ├── Bucket: Store B │ └── Bucket: Store C └── Date: 2023-09-03 ├── Bucket: Store A ├── Bucket: Store B └── Bucket: Store C In this setup, if you want to analyze orders from September 2, 2023, you directly go to that day\u0026rsquo;s folder. Then, if you\u0026rsquo;re interested in orders from Store B on that day, you only look at the bucket for Store B. This layered organization reduces the amount of data that needs to be scanned and processed, leading to faster query and join performance.\nTip:\nColumns chosen for partitioning should generally be those frequently used in WHERE clauses for filtering data, and should have low cardinality to avoid creating too many small partitions, which can hurt performance. Avoid partitioning by unique IDs like user IDs or phone numbers.\nTip:\nBucketing is especially effective when the bucketed column is used in GROUP BY or SORT BY clauses in your queries, as it can optimize these operations significantly. Aim for a bucket size that is a multiple of the HDFS block size or around 1GB, using the formula: Table size / Number of buckets \u0026gt;= HDFS block size or table size / 1 GB.\nPartitioning Operations # Partitioning in Hive involves dividing a table into smaller parts based on the values of one or more partition keys. This allows Hive to retrieve only the relevant partitions during a query, significantly reducing the amount of data scanned and improving query performance.\nCreating a Partitioned Table # Let\u0026rsquo;s look at an example of creating a partitioned table. In this case, we\u0026rsquo;ll partition a sales table by sales_date.\nNote:\nPrior to this below step, I am assuming you have already done all the steps prior to this given step of this blog post. Therefore you have a ready Hive service running on your Hadoop cluster, to be able to perform the following steps.\ncreate database db_partition; CREATE TABLE IF NOT EXISTS db_partition.sales_partitioned_by_date ( sales_id INT, country STRING, product_id INT, product_name STRING, quantity INT, unit_price FLOAT ) PARTITIONED BY (sales_date DATE) ROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39; LINES TERMINATED BY \u0026#39;\\n\u0026#39; STORED AS ORC; Tip: Notice that the partition column (sales_date in this example) is defined separately in the PARTITIONED BY clause and is not included in the column definitions within the parentheses before it. However, the sales_date field will still be part of the table columns to be queried just like all other fields. Static and Dynamic Partitioning # Hive supports two main types of partitioning: static and dynamic.\nStatic Partitioning: In static partitioning, you manually create each partition and explicitly specify the partition when loading data. This is suitable when you know the partition values beforehand. Dynamic Partitioning: Hive automatically creates partitions based on the data being inserted. The CREATE TABLE syntax is the same as for static partitioning. To enable dynamic partitioning, you need to set the following properties: set hive.exec.dynamic.partition=true; set hive.exec.dynamic.partition.mode=nonstrict; When dynamic partitioning is enabled, Hive will automatically infer and create new partitions as new partition key values are encountered during data loading.\nTip:\nWhen using dynamic partitioning, ensure that the partition column(s) are placed last in the INSERT statement\u0026rsquo;s SELECT clause or VALUES clause.\nWhen to use which? # Static Partitioning: Best used when you have a well-defined and relatively small number of partitions, and when data loading processes are predictable. It requires more upfront setup but can be more performant if partitions are well-managed. Dynamic Partitioning: More flexible and less development effort, especially when dealing with data where partition values are not known in advance or are numerous. However, dynamic partitioning can be slower than static partitioning, and there are limits to the number of dynamic partitions that can be created (default is 100 per node). Exceeding this limit will result in an error. Insert Data into Partitioned Table # Let\u0026rsquo;s see how to insert data into a dynamically partitioned table. Key points to remember:\nUse the INSERT INTO table PARTITION (partition_column) syntax. Ensure partition column values are provided last in the VALUES clause.INSERT INTO db_partition.sales_partitioned_by_date PARTITION (sales_date) VALUES (100001, \u0026#39;UK\u0026#39;, 2134562, \u0026#39;Electric Guitar\u0026#39;, 2, 2599.99, \u0026#39;2020-04-20\u0026#39;), (100002, \u0026#39;USA\u0026#39;, 2134563, \u0026#39;Acoustic Guitar\u0026#39;, 1, 1599.99, \u0026#39;2020-02-19\u0026#39;), (100003, \u0026#39;USA\u0026#39;, 2134563, \u0026#39;Acoustic Guitar\u0026#39;, 2, 1599.99, \u0026#39;2020-02-19\u0026#39;), (100004, \u0026#39;TR\u0026#39;, 2134563, \u0026#39;Bass Guitar\u0026#39;, 1, 1199.99, \u0026#39;2020-03-19\u0026#39;), (100005, \u0026#39;FR\u0026#39;, 2133563, \u0026#39;Drum Set\u0026#39;, 1, 899.99, \u0026#39;2020-04-11\u0026#39;), (100006, \u0026#39;UK\u0026#39;, 2134563, \u0026#39;Keyboard\u0026#39;, 1, 1359.99, \u0026#39;2020-04-14\u0026#39;), (100007, \u0026#39;USA\u0026#39;, 2134513, \u0026#39;Electric Bass\u0026#39;, 1, 699.99, \u0026#39;2020-04-20\u0026#39;), (100008, \u0026#39;TR\u0026#39;, 2134560, \u0026#39;Synthesizer\u0026#39;, 1, 1489.99, \u0026#39;2020-03-19\u0026#39;), (100009, \u0026#39;UK\u0026#39;, 2134569, \u0026#39;Violin\u0026#39;, 1, 465.00, \u0026#39;2020-04-11\u0026#39;), (100010, \u0026#39;FR\u0026#39;, 2134562, \u0026#39;Piano\u0026#39;, 1, 895.99, \u0026#39;2020-04-14\u0026#39;); select count(1) from db_partition.sales_partitioned_by_date; +------+ | _c0 | +------+ | 10 | +------+ List Partitions and Query Data # You can list the partitions of a Hive table in a few ways. One method is to directly look at the HDFS directory structure:\nhdfs dfs -ls /user/hive/warehouse/db_partition.db/sales_partitioned_by_date Found 5 items drwxr-xr-x - root supergroup 0 2025-02-16 12:26 /user/hive/warehouse/db_partition.db/sales_partitioned_by_date/sales_date=2020-02-19 drwxr-xr-x - root supergroup 0 2025-02-16 12:26 /user/hive/warehouse/db_partition.db/sales_partitioned_by_date/sales_date=2020-03-19 drwxr-xr-x - root supergroup 0 2025-02-16 12:26 /user/hive/warehouse/db_partition.db/sales_partitioned_by_date/sales_date=2020-04-11 drwxr-xr-x - root supergroup 0 2025-02-16 12:26 /user/hive/warehouse/db_partition.db/sales_partitioned_by_date/sales_date=2020-04-14 drwxr-xr-x - root supergroup 0 2025-02-16 12:26 /user/hive/warehouse/db_partition.db/sales_partitioned_by_date/sales_date=2020-04-20 Alternatively, you can use the Hive CLI command:\nSHOW partitions db_partition.sales_partitioned_by_date; +------------------------+ | partition | +------------------------+ | sales_date=2020-02-19 | | sales_date=2020-03-19 | | sales_date=2020-04-11 | | sales_date=2020-04-14 | | sales_date=2020-04-20 | +------------------------+ To query data from a specific partition, you can include a WHERE clause that filters on the partition column:\nSELECT * FROM db_partition.sales_partitioned_by_date WHERE sales_date = \u0026#39;2020-04-11\u0026#39; LIMIT 10; +-------------------------------------+------------------------------------+---------------------------------------+-----------------------------------------+-------------------------------------+---------------------------------------+---------------------------------------+ | sales_partitioned_by_date.sales_id | sales_partitioned_by_date.country | sales_partitioned_by_date.product_id | sales_partitioned_by_date.product_name | sales_partitioned_by_date.quantity | sales_partitioned_by_date.unit_price | sales_partitioned_by_date.sales_date | +-------------------------------------+------------------------------------+---------------------------------------+-----------------------------------------+-------------------------------------+---------------------------------------+---------------------------------------+ | 100005 | FR | 2133563 | Drum Set | 1 | 899.99 | 2020-04-11 | | 100009 | UK | 2134569 | Violin | 1 | 465.0 | 2020-04-11 | +-------------------------------------+------------------------------------+---------------------------------------+-----------------------------------------+-------------------------------------+---------------------------------------+---------------------------------------+ Bucketing # Bucketing is another data organization technique in Hive. While partitioning divides data into different directories, bucketing further divides data within partitions (or within a table if it\u0026rsquo;s not partitioned) into multiple files called buckets.\nBucketing is beneficial for:\nImproved query performance: Especially for queries involving joins and sampling. Data organization: Data is pre-sorted and organized within each bucket. To use bucketing, you must specify the number of buckets when creating the table and enable bucketing enforcement:\nset hive.enforce.bucketing = true; This setting ensures that Hive respects bucketing when writing data into the table.\nPartitioning + Bucketing Example # Let\u0026rsquo;s dive into a practical example that demonstrates the combined power of partitioning and bucketing. In this example, we will use the MovieLens dataset to analyze movie ratings. Our business needs require us to efficiently query the most popular movies of a month with the lowest possible latency. To achieve this, we will create a Hive table that is both partitioned and bucketed.\nDownload Datasets # First, we need to download the MovieLens datasets (u.data and u.item) from the provided URL using wget command into our cluster-master container shell where we run the Hive services from:\nMake sure you download it to the folder where you map the docker volume (usr/local/hadoop/namenode/), if you would like to access the dataset even the container gets restarted.\nroot@cluster-master:/# cd usr/local/hadoop/namenode/ mkdir hive_datasets cd hive_datasets/ wget -O u.item https://raw.githubusercontent.com/nacisimsek/Data_Engineering/refs/heads/main/Datasets/u.item wget -O u.data https://raw.githubusercontent.com/nacisimsek/Data_Engineering/refs/heads/main/Datasets/u.data To understand the structure and delimiters of these datasets, we can use the head command:\nroot@cluster-master:/usr/local/hadoop/namenode/hive_datasets# head u.item movieid|movietitle|releasedate|videoreleasedate|IMDbURL|unknown|Action|Adventure|Animation|Children\u0026#39;s|Comedy|Crime|Documentary|Drama|Fantasy|Film-Noir|Horror|Musical|Mystery|Romance|Sci-Fi|Thriller|War|Western 1|ToyStory(1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0 2|GoldenEye(1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?GoldenEye%20(1995)|0|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0 3|FourRooms(1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995)|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0 4|GetShorty(1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Get%20Shorty%20(1995)|0|1|0|0|0|1|0|0|1|0|0|0|0|0|0|0|0|0|0 5|Copycat(1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Copycat%20(1995)|0|0|0|0|0|0|1|0|1|0|0|0|0|0|0|0|1|0|0 ... root@cluster-master:/usr/local/hadoop/namenode/hive_datasets# head u.data user_id\titem_id\trating\ttimestamp 196\t242\t3\t881250949 186\t302\t3\t891717742 22\t377\t1\t878887116 244\t51\t2\t880606923 166\t346\t1\t886397596 298\t474\t4\t884182806 ... Now, let\u0026rsquo;s proceed to load these datasets into Hive tables. Start by launching the Beeline client and creating a database named movies:\ncreate database if not exists movielens; Load Data into Respective Hive Tables # First, we will load the u.data dataset into a Hive table named ratings.\nCreate Hive DB/Table and Load Data # To load the ratings data, we first create an external table movielens.ratings matching the structure of u.data. Note that the delimiter is tab (\\t) and we skip the header line.\nCREATE TABLE IF NOT EXISTS movielens.ratings ( user_id INT, item_id INT, rating INT, rating_time BIGINT ) ROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;\\t\u0026#39; LINES TERMINATED BY \u0026#39;\\n\u0026#39; STORED AS TEXTFILE TBLPROPERTIES (\u0026#39;skip.header.line.count\u0026#39;=\u0026#39;1\u0026#39;); Then, load the data from the local file system into the movielens.ratings table:\nload data local inpath \u0026#39;/usr/local/hadoop/namenode/hive_datasets/u.data\u0026#39; into table movielens.ratings; Verify the data load by selecting a few rows and checking the total row count and distinct user count:\nselect * from movielens.ratings limit 4; +------------------+------------------+-----------------+----------------------+ | ratings.user_id | ratings.item_id | ratings.rating | ratings.rating_time | +------------------+------------------+-----------------+----------------------+ | 196 | 242 | 3 | 881250949 | | 186 | 302 | 3 | 891717742 | | 22 | 377 | 1 | 878887116 | | 244 | 51 | 2 | 880606923 | +------------------+------------------+-----------------+----------------------+ select count(1) from movielens.ratings; +---------+ | _c0 | +---------+ | 100000 | +---------+ select count(distinct user_id) from movielens.ratings; +------+ | _c0 | +------+ | 943 | +------+ Next, load the u.item dataset into a Hive table named movies.\nCreate the movies table in the movielens database, matching the structure of u.item. Note that the delimiter is pipe (|) and we skip the header line.\ncreate table if not exists movielens.movies ( movieid int, movietitle string, releasedate string, videoreleasedate string, IMDbURL string, unknown tinyint, Action tinyint, Adventure tinyint, Animation tinyint, Childrens tinyint, Comedy tinyint, Crime tinyint, Documentary tinyint, Drama tinyint, Fantasy tinyint, FilmNoir tinyint, Horror tinyint, Musical tinyint, Mystery tinyint, Romance tinyint, SciFi tinyint, Thriller tinyint, War tinyint, Western tinyint) row format delimited fields terminated by \u0026#39;|\u0026#39; lines terminated by \u0026#39;\\n\u0026#39; stored as textfile tblproperties(\u0026#39;skip.header.line.count\u0026#39;=\u0026#39;1\u0026#39;); Load data from the local file system into the movielens.movies table:\nload data local inpath \u0026#39;/usr/local/hadoop/namenode/hive_datasets/u.item\u0026#39; into table movielens.movies; Verify the data load by selecting a few rows and checking the total row count:\nselect movieid, movietitle, releasedate, imdburl from movielens.movies limit 5; +----------+------------------+--------------+----------------------------------------------------+ | movieid | movietitle | releasedate | imdburl | +----------+------------------+--------------+----------------------------------------------------+ | 1 | ToyStory(1995) | 01-Jan-1995 | http://us.imdb.com/M/title-exact?Toy%20Story%20(1995) | | 2 | GoldenEye(1995) | 01-Jan-1995 | http://us.imdb.com/M/title-exact?GoldenEye%20(1995) | | 3 | FourRooms(1995) | 01-Jan-1995 | http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995) | | 4 | GetShorty(1995) | 01-Jan-1995 | http://us.imdb.com/M/title-exact?Get%20Shorty%20(1995) | | 5 | Copycat(1995) | 01-Jan-1995 | http://us.imdb.com/M/title-exact?Copycat%20(1995) | +----------+------------------+--------------+----------------------------------------------------+ 5 rows selected (0.189 seconds) select count(1) from movielens.movies; +-------+ | _c0 | +-------+ | 1682 | +-------+ Now that we have both ratings and movies data loaded into Hive, we can create our partitioned and bucketed table.\nCreate Partitioned and Bucketed Hive Table # To efficiently query popular movies by month, create the movielens.movie_ratings table partitioned by review_year and review_month, and bucketed by movietitle:\nCREATE TABLE IF NOT EXISTS movielens.movie_ratings ( user_id INT, rating INT, rating_time BIGINT, movieid INT, movietitle STRING, videoreleasedate STRING, imdburl STRING ) PARTITIONED BY ( review_year INT, review_month INT ) CLUSTERED BY (movietitle) INTO 4 BUCKETS STORED AS ORC; Enable dynamic partitioning and bucketing for the data loading process:\nset hive.exec.dynamic.partition=true; set hive.exec.dynamic.partition.mode=nonstrict; set hive.enforce.bucketing=true; Load Data into Partitioned \u0026amp; Bucketed Table by Joining Existing Hive Tables # Load data into the movielens.movie_ratings table by joining movielens.ratings and movielens.movies tables. The data will be dynamically partitioned by review_year and review_month:\ninsert overwrite table movielens.movie_ratings PARTITION(review_year, review_month) select r.user_id, r.rating, r.rating_time, m.movieid, m.movietitle, m.videoreleasedate, m.imdburl, YEAR(from_unixtime(r.rating_time, \u0026#39;yyyy-MM-dd\u0026#39;)) as review_year, MONTH(from_unixtime(r.rating_time, \u0026#39;yyyy-MM-dd\u0026#39;)) as review_month from movielens.ratings r join movielens.movies m on r.item_id = m.movieid; Perform Checks and Queries on Partitioned \u0026amp; Bucketed Final Hive Table # Verify the data loading and partitioning by checking the row count and listing partitions:\nselect count(1) from movielens.movie_ratings; +---------+ | _c0 | +---------+ | 100000 | +---------+ We can describe our table to see its fields, their data type, and see if there is partitioning in place, and if yes, on which columns the partitioning is performed:\ndescribe movielens.movie_ratings; +--------------------------+------------+----------+ | col_name | data_type | comment | +--------------------------+------------+----------+ | user_id | int | | | rating | int | | | rating_time | bigint | | | movieid | int | | | movietitle | string | | | videoreleasedate | string | | | imdburl | string | | | review_year | int | | | review_month | int | | | | NULL | NULL | | # Partition Information | NULL | NULL | | # col_name | data_type | comment | | review_year | int | | | review_month | int | | +--------------------------+------------+----------+ 14 rows selected (0.113 seconds) We can list the created partitions, which are basically different folder structure Hive generates on HDFS:\nshow partitions movielens.movie_ratings; +-----------------------------------+ | partition | +-----------------------------------+ | review_year=1997/review_month=10 | | review_year=1997/review_month=11 | | review_year=1997/review_month=12 | | review_year=1997/review_month=9 | | review_year=1998/review_month=1 | | review_year=1998/review_month=2 | | review_year=1998/review_month=3 | | review_year=1998/review_month=4 | +-----------------------------------+ It is always nice to use third party tools to access quick visualization of your underlying servers. This time, I will view the partitioning folder structure of our table from Big Data Plugin of Intellij where I connected to the underlying HDFS to see the partitioning folders created by Hive:\nSame can also be viewed on the Namenode UI\u0026rsquo;s Browse Directory menu where you can browse the HDFS:\nWe can see the partitioning works correctly by performing distinct query on review_year and review_month fields of our movielens.movie_ratings table:\nselect distinct (review_year, review_month) from movielens.movie_ratings; +--------------------------+ | _c0 | +--------------------------+ | {\u0026#34;col1\u0026#34;:1997,\u0026#34;col2\u0026#34;:9} | | {\u0026#34;col1\u0026#34;:1997,\u0026#34;col2\u0026#34;:10} | | {\u0026#34;col1\u0026#34;:1997,\u0026#34;col2\u0026#34;:11} | | {\u0026#34;col1\u0026#34;:1997,\u0026#34;col2\u0026#34;:12} | | {\u0026#34;col1\u0026#34;:1998,\u0026#34;col2\u0026#34;:1} | | {\u0026#34;col1\u0026#34;:1998,\u0026#34;col2\u0026#34;:2} | | {\u0026#34;col1\u0026#34;:1998,\u0026#34;col2\u0026#34;:3} | | {\u0026#34;col1\u0026#34;:1998,\u0026#34;col2\u0026#34;:4} | +--------------------------+ 8 rows selected (32.082 seconds) Now, let\u0026rsquo;s run queries to find the top rated movies in April 1998.\nTop rated counts movies in April 1998 # Find the top 20 most rated movies in April 1998:\nSELECT COUNT(*) AS total_count, movietitle FROM movielens.movie_ratings WHERE review_year = 1998 AND review_month = 4 GROUP BY movietitle ORDER BY total_count DESC LIMIT 20; +--------------+------------------------------+ | total_count | movietitle | +--------------+------------------------------+ | 63 | Titanic(1997) | | 52 | AirForceOne(1997) | | 50 | Contact(1997) | | 49 | FullMonty,The(1997) | | 49 | StarWars(1977) | | 42 | GoodWillHunting(1997) | | 41 | LiarLiar(1997) | | 41 | EnglishPatient,The(1996) | | 39 | AsGoodAsItGets(1997) | | 39 | ConspiracyTheory(1997) | | 37 | Scream(1996) | | 36 | ToyStory(1995) | | 36 | Fargo(1996) | | 36 | ReturnoftheJedi(1983) | | 35 | L.A.Confidential(1997) | | 34 | ChasingAmy(1997) | | 34 | Godfather,The(1972) | | 33 | Braveheart(1995) | | 33 | StarshipTroopers(1997) | | 33 | SilenceoftheLambs,The(1991) | +--------------+------------------------------+ Top average rated movies in April 1998 # Find the top 20 highest average rated movies in April 1998:\nSELECT AVG(rating) AS avg_rating, COUNT(*) AS total_count, movietitle FROM movielens.movie_ratings WHERE review_year = 1998 AND review_month = 4 GROUP BY movietitle ORDER BY avg_rating DESC LIMIT 20; +-------------+--------------+-------------------------------------------+ | avg_rating | total_count | movietitle | +-------------+--------------+-------------------------------------------+ | 5.0 | 3 | CelluloidCloset,The(1995) | | 5.0 | 1 | Boys,Les(1997) | | 5.0 | 1 | Flirt(1995) | | 5.0 | 1 | FreeWilly2:TheAdventureHome(1995) | | 5.0 | 1 | DeltaofVenus(1994) | | 5.0 | 1 | CutthroatIsland(1995) | | 5.0 | 1 | DunstonChecksIn(1996) | | 5.0 | 2 | Diexueshuangxiong(Killer,The)(1989) | | 5.0 | 1 | Lassie(1994) | | 5.0 | 1 | Innocents,The(1961) | | 5.0 | 1 | Stalingrad(1993) | | 5.0 | 1 | FearofaBlackHat(1993) | | 5.0 | 1 | Trust(1990) | | 5.0 | 1 | BoxingHelena(1993) | | 5.0 | 1 | DavyCrockett,KingoftheWildFrontier(1955) | | 5.0 | 1 | BitterSugar(AzucarAmargo)(1996) | | 5.0 | 1 | BlueSky(1994) | | 5.0 | 1 | Daylight(1996) | | 5.0 | 2 | Prefontaine(1997) | | 5.0 | 1 | 8Seconds(1994) | +-------------+--------------+-------------------------------------------+ These queries demonstrate how partitioning by year and month and bucketing by movie title can help optimize data retrieval and analysis for time-based and movie-centric queries for low latency access to popular movie data.\n","date":"2024-06-01","permalink":"/posts/20240601-hive-partitioning/","section":"Posts","summary":"This article is about how to perform partitioning and bucketing on Hive Tables","title":"Hive Partitioning and Bucketing"},{"content":"Hive Deployment and Operations # In the previous article, we deployed our Hadoop cluster on docker, accessed both HDFS and YARN UIs, and performed some simple file operations on HDFS.\nIn this article, we will deploy Hive services on the same Hadoop cluster and perform various operations on it to learn its basics, usage, and advantages for our data operations. If you directly opened this article without setting up your Docker environment, I suggest you visit that article to deploy your cluster first.\nImportant:\nSince the image used in this experiment is built with the needed Hive services included, we can run Hive services in this cluster. However, if you are working on your own cluster built on top of a different image, the operations/commands used in this article may not be compatible with your environment.\nIntroduction to Hive # Welcome to the world of Hive! If you\u0026rsquo;re new to this tool, don\u0026rsquo;t worry—by the end of this post, you\u0026rsquo;ll clearly understand what Hive is, why it was created, and how it works. Let\u0026rsquo;s get started!\nWhat is Hive? # Hive is like a friendly translator that helps you talk to your big data. Imagine you have a massive library filled with books (data), but they\u0026rsquo;re all written in different languages. You want to ask questions about the books but don\u0026rsquo;t speak those languages. Hive steps in to help! It allows you to use SQL (Structured Query Language), which is a common language for databases, to interact with your data stored in Hadoop, a framework for storing and processing large data sets.\nIt allows querying the data stored on HDFS with a SQL-like language. Created by Facebook. Handed over Apache community. It processes structured data that can be stored in a table. The mode in which it works is batch processing, not real-time (streaming data). It uses engines like MapReduce, Tez, or Spark. It supports many different file formats: Parquet, Sequence, ORC, Text, etc. Important:\nHive is not a database. It is unsuitable for operational database needs (OLTP) since it focuses on heavy analytics queries. It is unsuitable for operational database needs (OLTP) for row-based insert, update, and delete or interactive queries. The query takes a reasonable amount of time. Converts query to MapReduce/Tez code gets resources from YARN and starts the operation. HiveQL is not standard SQL. Should not expect everything as in SQL. Why was Hive Built? # In the early days of Hadoop, data analysts and scientists had to write complex and lengthy MapReduce Java programs to analyze data. This was time-consuming, error-prone, and required a deep understanding of programming languages like Java or Python. Hive was built to simplify this process. It provides a familiar SQL-like interface that allows users to query and analyze data without writing complex code. This makes it easier for data analysts, scientists, and even business users to work with Big Data without requiring extensive programming knowledge.\nHow does Hive Work? # Hive works by converting your SQL queries into a series of MapReduce jobs that run on Hadoop. Let’s break this down with a simple example: Imagine you run a bookstore and have a massive spreadsheet (your data) that lists all your books, their authors, prices, and sales numbers. You want to discover which books by a specific author sold the most copies last year. With Hive, you can write a simple SQL query like:\nSELECT title, SUM(sales) FROM books WHERE author = \u0026#39;J.K. Rowling\u0026#39; AND year = 2022 GROUP BY title ORDER BY SUM(sales) DESC; When you run this query, Hive translates it into a series of steps that Hadoop understands (MapReduce jobs), processes the data, and then gives you the results. This way, you don’t need to worry about the complex underlying processes; you get the information you need quickly and efficiently. In summary, Hive acts as a bridge between you and your data, making it easier to ask questions and get answers without needing to dive into the complexities of Hadoop. It’s like having a knowledgeable assistant who can find the information you need and present it in a way that’s easy to understand.\nHive Architecture # The following are the major components of Apache Hive Architecture.\nMetastore (RDBMS): Think of the Metastore as a library catalog. It stores information about the data, like where it\u0026rsquo;s located, what it looks like, and how it\u0026rsquo;s organized. This helps Hive keep track of the data and make sense of it. In our cluster, it is PostgreSQL DB. Driver: The Driver is like a manager. It creates a session, receives instructions from the user, and monitors the execution process. It also keeps track of the metadata (information about the data) while running queries. Compiler: The Compiler is like a translator. It takes the HiveQL query (a question written in Hive\u0026rsquo;s language) and converts it into a plan that Hadoop MapReduce can understand. It breaks down the query into smaller steps and creates a roadmap for execution. Optimizer: The Optimizer is like a performance coach. It examines the plan created by the Compiler and finds ways to make it more efficient. For example, it might combine multiple steps into one or rearrange them to get the best results. Executor: The Executor is like a team leader. It takes the optimized plan and works with Hadoop to execute it. It ensures that all the necessary tasks are completed and in the right order. Thrift Server and CLI/UI: The Thrift Server is like a receptionist. External clients can interact with Hive using standard protocols like JDBC or ODBC. The CLI (Command-Line Interface) and UI (User Interface) allow users to run HiveQL queries and interact with Hive directly. Hive Operations # Without further theory, let’s dive into our hands-on exercises where we start hive services, deploy the metadata database and tables in the underlying RDBMS (it is PostgreSQL in our cluster), create Hive databases and tables, insert data into our tables, see how the data in these tables reflect in the underlying storage layer HDFS, learn the difference between internal and external table terms, play with partitioning and bucketing, and finally cover the performance optimization tips.\nStarting Hive Services # Check if the containers of our Hadoop cluster are up and running. See this chapter for instructions on deploying this cluster.\ndocker ps --format \u0026#39;table {{.ID}}\\t{{.Names}}\\t{{.Status}}\u0026#39; CONTAINER ID NAMES STATUS 6ad193be83c3 cluster-slave-1 Up 12 days d04cd3f43266 cluster-slave-2 Up 12 days fa725f0c0bd9 cluster-master Up 12 days 02571464b056 postgresql Up 12 days Logging into the shell of the container cluster-master\ndocker exec -it cluster-master bash Hive Schema Initialization # Initialize the Hive metastore schema in a PostgreSQL database\nImportant:\nSchema initialization only needs to be performed for the first run of the Hive services. Once all the metadata tables are ready on Postgres, you need not initialize them again.\nschematool -initSchema -dbType postgres The output should look like this below.\nMetastore connection URL:\tjdbc:postgresql://postgresql:5432/metastore Metastore Connection Driver :\torg.postgresql.Driver Metastore connection User:\tpostgres Starting metastore schema initialization to 3.1.0 Initialization script hive-schema-3.1.0.postgres.sql Initialization script completed schemaTool completed We can verify on PostgreSQL that the database metastore is created with all its metadata tables:\nStarting Hive Services # We will then start the Hive services, which include:\nHive Metastore: A central repository that stores metadata about Hive tables, partitions, and schemas. HiveServer2: A service that allows clients to execute queries against Hive and retrieve results. $HADOOP_HOME/start-hive.sh Services starting up. Waiting for 60 seconds... Hive Metastore and HiveServer2 services have been started successfully. Connect Beeline HiveQL CLI # After the Hive service is started, we will connect to it using the Beeline CLI (Command Line Interface).\nThis command will connect us to a Hive server running on \u0026ldquo;cluster-master\u0026rdquo; using the default port 10000, allowing us to interact with Hive and run HiveQL queries.\nbeeline -u jdbc:hive2://cluster-master:10000 Connecting to jdbc:hive2://cluster-master:10000 Connected to: Apache Hive (version 3.1.3) Driver: Hive JDBC (version 2.3.9) Transaction isolation: TRANSACTION_REPEATABLE_READ Beeline version 2.3.9 by Apache Hive Note:\nIf you encounter the issue below when trying to connect to Beeline CLI, it’s most probably related to your Postgres container\u0026rsquo;s volume access.\nConnecting to jdbc:hive2://cluster-master:10000 Could not open connection to the HS2 server. Please check the server URI and if the URI is correct, then ask the administrator to check the server status. Error: Could not open client transport with JDBC Uri: jdbc:hive2://cluster-master:10000: java.net.ConnectException: Connection refused (Connection refused) (state=08S01,code=0) To fix it, either update the volume settings of the docker compose file and mount the container volumes to a local volume, or make sure to start containers with docker-compose up -d command executed by a root user or another user which has access to the local volume folders created by docker.\nConnect Hive2 JDBC Through SQL Editor # If you like to work on a SQL client rather than performing SQL queries on CLI, there is also an option to connect Hive via a SQL editor through JDBC. Here are the details to be used when connecting to Hive through a SQL Editor (ex: DBeaver):\nCreating a Hive Database and a Table # We are now ready to perform our HiveQL database and table operations on Beeline.\nList Databases and Tables # Below command is used to list the available databases:\nshow databases; The output will be shown as follows:\nINFO : Compiling command(queryId=root_20240818123321_a5886e81-31b2-493d-93e1-88e05b7431f7): show databases INFO : Concurrency mode is disabled, not creating a lock manager INFO : Semantic Analysis Completed (retrial = false) INFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:database_name, type:string, comment:from deserializer)], properties:null) INFO : Completed compiling command(queryId=root_20240818123321_a5886e81-31b2-493d-93e1-88e05b7431f7); Time taken: 0.016 seconds INFO : Concurrency mode is disabled, not creating a lock manager INFO : Executing command(queryId=root_20240818123321_a5886e81-31b2-493d-93e1-88e05b7431f7): show databases INFO : Starting task [Stage-0:DDL] in serial mode INFO : Completed executing command(queryId=root_20240818123321_a5886e81-31b2-493d-93e1-88e05b7431f7); Time taken: 0.017 seconds INFO : OK INFO : Concurrency mode is disabled, not creating a lock manager +----------------+ | database_name | +----------------+ | default | +----------------+ 1 row selected (0.119 seconds) Note:\nAs seen above, with the command output, also many other logs get pinted. Simply use the below command to turn the logging function off for this session:\nset hive.server2.logging.operation.level=NONE; If we would like to turn logging of system-wide, this needs to be set on the below config file of hive:\n./usr/local/hive/conf/hive-site.xml\nTo show the tables, simply use as below:\nshow tables; +-----------+ | tab_name | +-----------+ +-----------+ No rows selected (0.03 seconds) Loading Data into a Hive Table # We now have the environment ready to work with Hive databases and tables to create our own table and perform queries on it.\nTo perform our tests, we will be using CSV files to initiate data into our internal and external tables and see how Hive make use of data when the table is generated as internal table or external table.\nIn addition to creating internal and external tables based on a CSV dataset—which we’ll focus on in this chapter—there are several other methods to create and populate Hive tables:\nManually Creating a Table and Inserting Data with INSERT INTO Inserting Data from Another Table with CREATE TABLE AS SELECT Creating a Table Using SHOW CREATE TABLE Creating an Empty Table Based on Another Table with CREATE TABLE LIKE Creating Tables with Apache Spark For more information about the file formats Hive can read and write, you can check here.\nFor internal Hive tables, data itself and its metadata are both managed by Hive. If the internal table is dropped, the metadata and the table data that is kept in HDFS is deleted. However, for external tables, only the metadata of the table is managed by Hive, the data itself resides in its predefined HDFS location, even if the table is dropped.\nInternal vs. External Tables: Data Movement # Internal (Managed) Tables # LOAD DATA INPATH: Action: Moves the data file from its current HDFS location to the table’s directory in the Hive warehouse. Result: The original data file is removed from its initial location after the move. LOAD DATA LOCAL INPATH: Action: Copies the data file from the local file system to the table’s directory in the Hive warehouse. Result: The original local data file remains intact. Dropping the Table: Both metadata and data files are deleted from Hive’s warehouse directory.\nExternal Tables # LOAD DATA INPATH: Action: Moves the data file from its current HDFS location to the external table’s specified directory. Result: The original data file is removed from its initial location after the move. LOAD DATA LOCAL INPATH: Action: Copies the data file from the local file system to the external table’s specified directory. Result: The original local data file remains intact. Dropping the Table: Only the metadata is deleted. The data files remain at the external location in HDFS.\nInternal Hive Table Creation # Before proceding to create database and the respective table, first we download our dataset that we use to insert data into our Hive table. Because, before creating the table, we need to check our dataset, and collect some details about it as explained below, which the Hive table will use all these details to suit the table with that dataset.\nThere are also other ways to create tables by creating a table over another table by CTAS SQL commands or insert manual data into the created table. Since mostly the hive tables are created based on an existing dataset, we followed this way in this blog.\nDownload Dataset to Container Local # Login to the container bash:\ndocker exec -it cluster-master bash Download the dataset. Make sure you download it to the folder where you map the docker volume (usr/local/hadoop/namenode/), if you would like to access the dataset even the container gets restarted.\nwget -O employee.txt https://raw.githubusercontent.com/nacisimsek/Data_Engineering/main/Datasets/employee.txt Here is how our data looks like:\ncat employee.txt name|work_place|gender_age|skills_score Michael|Montreal,Toronto|Male,30|DB:80,Network:88 Will|Montreal|Male,35|Perl:85,Scala:82 Shelley|New York|Female,27|Python:80,Spark:95 Lucy|Vancouver|Female,57|Sales:89,HR:94 Based on the content of the data here are the informations that we need to collect to be used when creating its Hive table:\nHas the dataset a header? Yes, 1 line What are each field names and their data types? name STRING work_place ARRAY\u0026lt;STRING\u0026gt; gender_age STRUCT\u0026lt;gender:STRING,age:INT\u0026gt; skills_score MAP\u0026lt;STRING,INT\u0026gt; How (with which character) each field is separated? With Pipe character \u0026#39;|\u0026#39; How each row (line) is separated? With a newline character \u0026#39;\\n\u0026#39; Does any column consists collection data type representing key:value pairs? Yes, collections in one column is separated by comma \u0026#39;,\u0026#39; Map keys are terminated by colon \u0026#39;:\u0026#39; Put the file into HDFS # We wiil place the dataset employee.txt into HDFS manually, then based on its HDFS directory, we will create its Hive table in the next step . In the directory of the container where we downloaded the dataset, execute below commands:\nAs we clarified earlier different options of loading data into the HDFS table, there are 2 different options:\nLoading data from HDFS: LOAD DATA INPATH Loading data from Local: LOAD DATA LOCAL INPATH We will be using the first option LOAD DATA INPATH since our dataset will be loaded to HDFS already manually, as follows:\nCreate the HDFS directory for our dataset:\nhdfs dfs -mkdir -p /user/datasets Put the file into this created HDFS directory:\nhdfs dfs -put employee.txt /user/datasets See the HDFS directory to verify that the dataset has successfully been placed there:\nhdfs dfs -ls /user/datasets Found 1 items -rw-r--r-- 1 root supergroup 215 2024-11-23 17:02 /user/datasets/employee.txt Create Hive Database and the Table # From Beeline (you can also use tools like DBeaver to connect Hive and execute HiveSQL queries), we will create a database called hive_db and a table called wine:\ncreate database if not exists hive_db; Show the existing databases:\nshow databases; |database_name| |-------------| |default | |hive_db | Describe particular database to see its details:\ndescribe database hive_db; |db_name|comment|location |owner_name|owner_type|parameters| |-------|-------|---------------------------------------------------------|----------|----------|----------| |hive_db| |hdfs://cluster-master:9000/user/hive/warehouse/hive_db.db|root |USER | | Select the created database:\nuse hive_db; Create the Hive table:\nWe will need to create a table that matches the structure of our dataset. Given the data and the data types we have identified earlier, the CREATE TABLE statement will look like this:\nCREATE TABLE employee ( name STRING, work_place ARRAY\u0026lt;STRING\u0026gt;, gender_age STRUCT\u0026lt;gender:STRING, age:INT\u0026gt;, skills_score MAP\u0026lt;STRING, INT\u0026gt; ) ROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;|\u0026#39; COLLECTION ITEMS TERMINATED BY \u0026#39;,\u0026#39; MAP KEYS TERMINATED BY \u0026#39;:\u0026#39; LINES TERMINATED BY \u0026#39;\\n\u0026#39; STORED AS TEXTFILE TBLPROPERTIES (\u0026#39;skip.header.line.count\u0026#39;=\u0026#39;1\u0026#39;); Explanation:\nname STRING: The name of the employee. work_place ARRAY \u0026lt;STRING\u0026gt;: A list of workplaces, since some entries have multiple locations separated by commas (e.g., Montreal,Toronto). gender_age STRUCT\u0026lt;gender:STRING, age:INT\u0026gt;: A structure containing the gender and age, separated by a comma (e.g., Male,30). skills_score MAP\u0026lt;STRING, INT\u0026gt;: A map of skills to scores, with skills and scores separated by colons and multiple entries separated by commas (e.g., DB:80,Network:88). Delimiters and Formatting:\nFIELDS TERMINATED BY \u0026lsquo;|\u0026rsquo;: Fields are separated by a pipe (|). COLLECTION ITEMS TERMINATED BY \u0026lsquo;,\u0026rsquo;: Items in arrays, structs, and maps are separated by commas. MAP KEYS TERMINATED BY \u0026lsquo;:\u0026rsquo;: In maps, keys and values are separated by colons. LINES TERMINATED BY \u0026lsquo;\\n\u0026rsquo;: Each record is on a new line. STORED AS TEXTFILE: Specifies that the data is stored in a text file format. TBLPROPERTIES (\u0026lsquo;skip.header.line.count\u0026rsquo;=\u0026lsquo;1\u0026rsquo;): Skips the header line in your data file. Note: The TBLPROPERTIES (\u0026lsquo;skip.header.line.count\u0026rsquo;=\u0026lsquo;1\u0026rsquo;) property is essential here because your dataset includes a header row that you don’t want to import as data. Load Data into Table # Since our data file employee.txt is already in HDFS at /user/datasets/employee.txt, we can load it into our internal table using the LOAD DATA INPATH command:\nLOAD DATA INPATH \u0026#39;/user/datasets/employee.txt\u0026#39; INTO TABLE employee; Important:\nData Movement : This command will move the employee.txt file from its current HDFS location into the Hive table’s directory within the Hive warehouse. After this operation, the original file at /user/datasets/employee.txt will no longer exist.\nInternal Table : Since we are creating an internal (managed) table, Hive assumes responsibility for the data files. Dropping the table later will delete both the table metadata and the data files.\nVerify That the Data Is Loaded Correctly # We can run a simple SELECT query to check that your data has been loaded properly:\nSELECT * FROM employee; +----------------+-------------------------+-------------------------------+---------------------------+ | employee.name | employee.work_place | employee.gender_age | employee.skills_score | +----------------+-------------------------+-------------------------------+---------------------------+ | Michael | [\u0026#34;Montreal\u0026#34;,\u0026#34;Toronto\u0026#34;] | {\u0026#34;gender\u0026#34;:\u0026#34;Male\u0026#34;,\u0026#34;age\u0026#34;:30} | {\u0026#34;DB\u0026#34;:80,\u0026#34;Network\u0026#34;:88} | | Will | [\u0026#34;Montreal\u0026#34;] | {\u0026#34;gender\u0026#34;:\u0026#34;Male\u0026#34;,\u0026#34;age\u0026#34;:35} | {\u0026#34;Perl\u0026#34;:85,\u0026#34;Scala\u0026#34;:82} | | Shelley | [\u0026#34;New York\u0026#34;] | {\u0026#34;gender\u0026#34;:\u0026#34;Female\u0026#34;,\u0026#34;age\u0026#34;:27} | {\u0026#34;Python\u0026#34;:80,\u0026#34;Spark\u0026#34;:95} | | Lucy | [\u0026#34;Vancouver\u0026#34;] | {\u0026#34;gender\u0026#34;:\u0026#34;Female\u0026#34;,\u0026#34;age\u0026#34;:57} | {\u0026#34;Sales\u0026#34;:89,\u0026#34;HR\u0026#34;:94} | +----------------+-------------------------+-------------------------------+---------------------------+ We can perform queries that access nested fields within our complex data types.\nTo select individual fields from the gender_age struct:\nSELECT name, gender_age.gender AS gender, gender_age.age AS age FROM employee; +----------+---------+------+ | name | gender | age | +----------+---------+------+ | Michael | Male | 30 | | Will | Male | 35 | | Shelley | Female | 27 | | Lucy | Female | 57 | +----------+---------+------+ To get all workplaces or a specific workplace:\nAll workspaces:\nSELECT name, work_place FROM employee; +----------+-------------------------+ | name | work_place | +----------+-------------------------+ | Michael | [\u0026#34;Montreal\u0026#34;,\u0026#34;Toronto\u0026#34;] | | Will | [\u0026#34;Montreal\u0026#34;] | | Shelley | [\u0026#34;New York\u0026#34;] | | Lucy | [\u0026#34;Vancouver\u0026#34;] | +----------+-------------------------+ First workplace:\nSELECT name, work_place[0] AS primary_work_place FROM employee; +----------+---------------------+ | name | primary_work_place | +----------+---------------------+ | Michael | Montreal | | Will | Montreal | | Shelley | New York | | Lucy | Vancouver | +----------+---------------------+ Find the employees that Python skill is greter than 70.\nSELECT name, skills_score[\u0026#39;Python\u0026#39;] as Python FROM employee WHERE skills_score[\u0026#39;Python\u0026#39;] \u0026gt; 70; +----------+---------+ | name | python | +----------+---------+ | Shelley | 80 | +----------+---------+ Drop Database and its Table # Since the table was created as an internal (managed) table, and as it was mentioned earlier, Hive will be controlling both the table metadata and the data files on HDFS, therefore, the drop table command will cause the table metadata and the data files on HDFS to be deleted.\nSee the data on HDFS listed before dropping the table:\nhdfs dfs -ls /user/hive/warehouse/hive_db.db/employee Found 1 items -rw-r--r-- 1 root supergroup 215 2024-12-29 12:15 /user/hive/warehouse/hive_db.db/employee/employee.txt To drop a Hive table, we can simply make sure we are using the correct database with use \u0026lt;db\u0026gt; command we ran earlier. Then, perform below command to drop employee table:\ndrop table employee; Or, we can also mention DB name before the table name, to make sure we drop the correct table:\ndrop table hive_db.employee; Lets see same directory in HDFS after performing the drop operation:\nhdfs dfs -ls /user/hive/warehouse/hive_db.db/employee ls: `/user/hive/warehouse/hive_db.db/employee\u0026#39;: No such file or directory To drop the whole database, simply use drop command for database:\ndrop database hive_db; External Hive Table Creation # As it was explained earlier, for external tables, we define an HDFS location for the data to be kept for our external table. When a data is loaded to the table, the underlying data files are stored into this location. When we use local file, local file got copied to this HDFS location. However, when we use a file from HDFS, the file is moved from its original HDFS location to the table\u0026rsquo;s location.\nThe critical part is that, when the table is dropped, only the metadata that is being kept in the Hive Metastore got deleted. the data files on the table\u0026rsquo;s HDFS location remain intact.\nLet\u0026rsquo;s download the dataset to our cluster-master container local which we are going to use to create our external table\nwget -O customers.csv https://raw.githubusercontent.com/nacisimsek/Data_Engineering/refs/heads/main/Datasets/customers.csv See the content of our dataset:\nroot@cluster-master:/# head customers.csv customerId,customerFName,customerLName,customerEmail,customerPassword,customerStreet,customerCity,customerState,customerZipcode 1,Richard,Hernandez,XXXXXXXXX,XXXXXXXXX,6303 Heather Plaza,Brownsville,TX,78521 2,Mary,Barrett,XXXXXXXXX,XXXXXXXXX,9526 Noble Embers Ridge,Littleton,CO,80126 3,Ann,Smith,XXXXXXXXX,XXXXXXXXX,3422 Blue Pioneer Bend,Caguas,PR,00725 4,Mary,Jones,XXXXXXXXX,XXXXXXXXX,8324 Little Common,San Marcos,CA,92069 5,Robert,Hudson,XXXXXXXXX,XXXXXXXXX,\u0026#34;10 Crystal River Mall \u0026#34;,Caguas,PR,00725 6,Mary,Smith,XXXXXXXXX,XXXXXXXXX,3151 Sleepy Quail Promenade,Passaic,NJ,07055 7,Melissa,Wilcox,XXXXXXXXX,XXXXXXXXX,9453 High Concession,Caguas,PR,00725 8,Megan,Smith,XXXXXXXXX,XXXXXXXXX,3047 Foggy Forest Plaza,Lawrence,MA,01841 9,Mary,Perez,XXXXXXXXX,XXXXXXXXX,3616 Quaking Street,Caguas,PR,00725 Put the dataset to HDFS:\nhdfs dfs -put customers.csv /user/datasets hdfs dfs -ls /user/datasets Found 1 items -rw-r--r-- 1 root supergroup 953847 2025-01-12 23:19 /user/datasets/customers.csv Create the corresponding external Hive table:\nuse hive_db; CREATE EXTERNAL TABLE IF NOT EXISTS customers_ext ( customerId INT, customerFName STRING, customerLName STRING, customerEmail STRING, customerPassword STRING, customerStreet STRING, customerCity STRING, customerState STRING, customerZipcode STRING ) COMMENT \u0026#39;External table for customers dataset\u0026#39; ROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39; LINES TERMINATED BY \u0026#39;\\n\u0026#39; STORED AS TEXTFILE LOCATION \u0026#39;/user/external/customer\u0026#39; TBLPROPERTIES (\u0026#34;skip.header.line.count\u0026#34;=\u0026#34;1\u0026#34;); Load data to our new external table:\nLOAD DATA INPATH \u0026#39;/user/datasets/customers.csv\u0026#39; INTO TABLE customers_ext; As soon as we execute this command, our dataset /user/datasets/customers.csv on HDFS is moved to the table location /user/external/customer:\nhdfs dfs -ls /user/external/customer Found 1 items -rw-r--r-- 1 root supergroup 953847 2025-01-12 23:19 /user/external/customer/customers.csv hdfs dfs -ls /user/datasets --no file found, it got moved to external folder where the LOCATION of the external table set. select * from customers_ext limit 5; +---------------------------+------------------------------+------------------------------+------------------------------+---------------------------------+-------------------------------+-----------------------------+------------------------------+--------------------------------+ | customers_ext.customerid | customers_ext.customerfname | customers_ext.customerlname | customers_ext.customeremail | customers_ext.customerpassword | customers_ext.customerstreet | customers_ext.customercity | customers_ext.customerstate | customers_ext.customerzipcode | +---------------------------+------------------------------+------------------------------+------------------------------+---------------------------------+-------------------------------+-----------------------------+------------------------------+--------------------------------+ | 1 | Richard | Hernandez | XXXXXXXXX | XXXXXXXXX | 6303 Heather Plaza | Brownsville | TX | 78521 | | 2 | Mary | Barrett | XXXXXXXXX | XXXXXXXXX | 9526 Noble Embers Ridge | Littleton | CO | 80126 | | 3 | Ann | Smith | XXXXXXXXX | XXXXXXXXX | 3422 Blue Pioneer Bend | Caguas | PR | 00725 | | 4 | Mary | Jones | XXXXXXXXX | XXXXXXXXX | 8324 Little Common | San Marcos | CA | 92069 | | 5 | Robert | Hudson | XXXXXXXXX | XXXXXXXXX | \u0026#34;10 Crystal River Mall \u0026#34; | Caguas | PR | 00725 | +---------------------------+------------------------------+------------------------------+------------------------------+---------------------------------+-------------------------------+-----------------------------+------------------------------+--------------------------------+ 5 rows selected (0.221 seconds) Here the metastore data of our external table on postgres:\nselect * from public.\u0026#34;TBLS\u0026#34; t limit 5; |TBL_ID|CREATE_TIME |DB_ID|LAST_ACCESS_TIME|OWNER|OWNER_TYPE|RETENTION|SD_ID|TBL_NAME |TBL_TYPE |VIEW_EXPANDED_TEXT|VIEW_ORIGINAL_TEXT|IS_REWRITE_ENABLED| |------|-------------|-----|----------------|-----|----------|---------|-----|-------------|--------------|------------------|------------------|------------------| |8 |1.736.723.756|11 |0 |root |USER |0 |8 |customers_ext|EXTERNAL_TABLE| | |false | Lets drop our table and observe the metadata and the file on HDFS:\ndrop table customers_ext; The table data is still there on HDFS:\nroot@cluster-master:/# hdfs dfs -ls /user/external/customer Found 1 items -rw-r--r-- 1 root supergroup 953847 2025-01-12 23:37 /user/external/customer/customers.csv However, its metadata is removed on Hive Metastore:\nselect * from public.\u0026#34;TBLS\u0026#34; t limit 5; |TBL_ID|CREATE_TIME |DB_ID|LAST_ACCESS_TIME|OWNER|OWNER_TYPE|RETENTION|SD_ID|TBL_NAME |TBL_TYPE |VIEW_EXPANDED_TEXT|VIEW_ORIGINAL_TEXT|IS_REWRITE_ENABLED| |------|-------------|-----|----------------|-----|----------|---------|-----|-------------|--------------|------------------|------------------|------------------| In the next blog post, we will be performing other table operations, and also have a look at different file formatting and compression techniques, by comparing them with their data size, and also perform partitioning and bucketing while creating our tables to improve the query performance. Stay tuned.\n","date":"2024-06-01","permalink":"/posts/20240601-hive/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Hive Setup and Operations"},{"content":"","date":"2024-05-09","permalink":"/tags/datanode/","section":"Tags","summary":"","title":"Datanode"},{"content":"Hadoop Cluster Deployment and Operations # In this article, we will be deploying Hadoop cluster on our local Docker environment with 1 Master (namenode \u0026amp; datanode) and 2 slave nodes (datanodes only) and perform data operations. We will be storing data into HDFS, and observe its operations on Hadoop UI.\nIn the world of big data, managing and processing vast amounts of information efficiently has always been a challenge. Traditional systems struggled to keep up with the growing volume, velocity, and variety of data. This is where Hadoop, an open-source framework, revolutionized the landscape of data processing and storage.\nIntroduction to Hadoop # What is Hadoop? # Hadoop is designed to store and process large datasets across clusters of computers using a simple programming model. It is highly scalable, cost-effective, and fault-tolerant, making it ideal for handling big data.\nWhy was Hadoop Built? # Before Hadoop, traditional systems struggled with the limitations of single-node processing and storage, leading to issues like high costs, limited scalability, and poor fault tolerance. Hadoop was designed to address these challenges by:\nScalability : Easily adding more nodes to handle increasing data loads. Cost-Effectiveness : Using commodity hardware to build large clusters. Fault Tolerance : Ensuring data availability and reliability even if some nodes fail. High Throughput : Efficiently processing large datasets in parallel. Comparing to Legacy Systems\nTraditional data processing systems were often limited by their ability to scale and handle large, diverse datasets efficiently. They typically relied on expensive, high-end hardware and faced significant challenges in terms of fault tolerance and scalability. Hadoop, with its distributed architecture, addressed these limitations by providing a robust, scalable, and cost-effective solution for big data processing.\nWith Hadoop, data storage and the computation both handled on the nodes which consist the Hadoop cluster.\nHow does Hadoop Work? # Hadoop\u0026rsquo;s architecture is built around three main components: HDFS, MapReduce, and YARN.\nHDFS (Hadoop Distributed File System) : # Purpose : HDFS is designed to store large files by distributing them across multiple machines in a cluster.\nHow It Works : HDFS breaks down a large file into smaller blocks and stores them across different nodes in the cluster. This distribution allows for parallel processing and ensures data availability, even if some nodes fail.\nLogic : By spreading the data, HDFS provides high throughput and reliability, addressing the limitations of single-node storage systems.\nModules:\nNameNode: Manages HDFS metadata and namespace. These nodes does not store data, but they actually keep the metadata of the data that is kept in data nodes, such as which data nodes the data is splitted, what is its replication, etc\u0026hellip;\nDataNode: Stores actual HDFS data blocks. These are the nodes where the actual data blocks are kept. When user would like to read/write data from/to HDFS, after getting the metadata information from NameNode, client is communicating these nodes for data operations.\nSecondary/Standby NameNode: Periodically saves the merged namespace image to reduce NameNode load and also grant High Availability (HA) for the cluster.\nBelow is a video by Jesse Anderson where he is explaining how the data is kept as blocks in HDFS.\nMapReduce : # Purpose : MapReduce is the core processing engine of Hadoop, designed to process large datasets in parallel.\nHow It Works : It breaks down a task into two main functions: Map and Reduce.\nMap Function : Processes input data and converts it into a set of intermediate key-value pairs.\nReduce Function : Merges these intermediate values to produce the final output.\nLogic : This parallel processing model allows Hadoop to handle large-scale data analysis efficiently, overcoming the bottlenecks of traditional sequential processing.\nYARN (Yet Another Resource Negotiator) : # Purpose : YARN manages and allocates resources to various applications running in a Hadoop cluster. How It Works : It consists of a ResourceManager and NodeManagers. The ResourceManager allocates resources based on the needs of the applications, while NodeManagers monitor resources on individual nodes. Logic : YARN enhances Hadoop’s scalability and resource utilization, enabling multiple data processing engines to run simultaneously on a single cluster. Modules: ResourceManager: Manages resource allocation in the YARN ecosystem.\nNodeManager: Manages containers and resources on individual nodes in YARN.\nApplicationMaster: Manages the execution (scheduling and coordination) of a single application in YARN during the application lifecycle and got removed as soon as the application terminates.\nBelow is the representation of the job submission and its management on YARN\nWhat are the Disadvantages of Hadoop Comparing to Modern Data Systems? # Hadoop was a groundbreaking solution for big data, but modern data systems have emerged with enhancements that address many of its limitations. Here’s a look at some key disadvantages of Hadoop:\nComplexity in Management and Configuration: Requires specialized knowledge for setup, configuration, and maintenance, which can be complex and time-consuming. Performance and Latency: Primarily batch-oriented with high latency in processing large datasets. Resource Efficiency: Often resource-intensive with significant computational and storage demands. Flexibility and Ecosystem Integration: Limited flexibility with a strong focus on MapReduce and a more rigid ecosystem. Scalability and Elasticity: Adding new nodes is not fast or easy, requiring manual intervention and planning. Data Handling and Processing Capabilities: Computation and storage are tightly coupled, limiting flexibility in resource allocation. Deployment of the Cluster # We will deploy the cluster by using the following docker file:\nhttps://raw.githubusercontent.com/nacisimsek/Data_Engineering/main/Hadoop/docker-compose.yaml\n📝 Note:\nThe image which is being used in this docker-compose file is my multiarch built version of the image which was originally prepared by Veribilimiokulu. I had attended their data engineering bootcamp program and had a chance to learn many new skills around data engineering while also refreshing my existing knowledge. Many of the next blog posts in my website will be related to the hands-on experience I gained during this bootcamp, therefore, special thanks to them for helping us improve ourselves and also encouraging us to share our knowledge to others.\nSimply copy the docker compose file and execute below command to deploy the containers.\ndocker-compose up -d This will compose the following four containers:\ncluster-master cluster-slave-1 cluster-slave-2 postgresql List the containers and their status with the following command:\ndocker ps --format \u0026#39;table {{.ID}}\\t{{.Names}}\\t{{.Status}}\u0026#39; CONTAINER ID NAMES STATUS 362d93c0d28a cluster-slave-1 Up About an hour 5e69cc3072aa cluster-slave-2 Up About an hour bd3276aa0e7f cluster-master Up About an hour 63ea237d5907 postgresql Up About an hour After the containers are started, make sure each container has started the above mentioned HDFS and YARN specific modules successfully.\nTo check this, need to connect the shell of each container:\ndocker exec -it cluster-master bash Then perform below command to see the started modules (services):\nroot@cluster-master:/# jps 455 NameNode 637 Jps 110 GetConf Since this is our master node of YARN and HDFS, and also will be used as one of our data nodes in HDFS and a worker node for YARN, we need to make sure all below modules (services) to be running on it:\nResourceManager (YARN) NodeManager (YARN) DataNode (HDFS) Execute below commands to start these services if they have not been started:\nTo start NodeManager and ResourceManager:\n/usr/local/hadoop/sbin/start-yarn.sh To start DataNode:\n/usr/local/hadoop/sbin/hadoop-daemon.sh start datanode Finally, check if all modules have been started successfully:\nroot@cluster-master:/# jps 903 ResourceManager 455 NameNode 1815 Jps 1560 DataNode 1163 NodeManager 110 GetConf And this is how the slave nodes should look like:\nroot@cluster-slave-1:/# jps 496 DataNode 2017 Jps 1618 NodeManager 837 SecondaryNameNode We should be now accessing to the Hadoop NameNode Web UI (Port 9870) and YARN ResourceManager Web UI (Port 8088)\nPort 9870: Hadoop NameNode Web UI # You can access the namenode web UI from your browser: http://localhost:9870/\nPurpose : The web interface on port 9870 is the Hadoop NameNode Web UI. It is used for monitoring the HDFS (Hadoop Distributed File System). Functions : View HDFS Health : Provides an overview of the HDFS, including the health and status of the NameNode. Browse File System : Allows users to browse the HDFS directories and files. Check DataNode Status : Displays the status and details of all DataNodes in the cluster, including storage utilization and block distribution. Monitor Replication : Shows information about block replication and under-replicated blocks. View Logs : Access NameNode logs for troubleshooting and monitoring. Key Features : HDFS Overview : Presents a summary of the total and available storage. DataNodes Information : Details on each DataNode’s storage capacity, usage, and health. HDFS Metrics : Metrics on file system operations, such as read and write requests. 📝 Note:\nIf you do not see all three nodes listed as Datanode in above list, its most likely the DataNode service is stopped or should be restarted on those nodes. If so, you can connect to the respective container\u0026rsquo;s shell and restart DataNode service as follows:\ndocker exec -it \u0026lt;container_name\u0026gt; /bin/bash hdfs --daemon start datanode ❗️ Important:\nNormally in commercial systems, the master node should not be using as a DataNode, but here in this cluster, for testing purposes, we deployed the master node is also one of the DataNode.\nPort 8088: YARN ResourceManager Web UI # You can access the YARN resource manager web UI from your browser: http://localhost:8088/\nPurpose : The web interface on port 8088 is the YARN ResourceManager Web UI. It is used for managing and monitoring YARN (Yet Another Resource Negotiator), which handles resource allocation and job scheduling in the Hadoop cluster. Functions : Monitor Applications : Displays the status of running and completed applications (jobs) within the cluster. View Cluster Metrics : Provides metrics on resource usage, including memory and CPU utilization across the cluster. Track Application Logs : Allows users to access logs for individual applications, aiding in troubleshooting and performance analysis. Manage Nodes : Lists all the nodes in the cluster with details about their resource usage and health. Key Features : Application Overview : Summarizes the state, resource usage, and history of applications. Cluster Utilization : Shows real-time data on how resources are being utilized across the cluster. Node Management : Information on each NodeManager, including available and used resources. 📝 Note:\nIf you do not see all three nodes listed as Active Nodes in above page, its most likely the NodeManager service is stopped or should be restarted on those nodes. If so, you can connect to the respective container\u0026rsquo;s shell and restartNodeManager service as follows:\n\u0026gt; docker exec -it cluster-slave-2 /bin/bash root@cluster-slave-2:/# jps 480 DataNode 929 GetConf 1416 Jps 798 SecondaryNameNode /usr/local/hadoop/sbin/yarn-daemon.sh start nodemanager Cluster Operations # We will be performing operations on HDFS and YARN to get familiar with them.\nHDFS Operations # Download the CSV File to Local, which we will use to import to HDFS: wget https://raw.githubusercontent.com/nacisimsek/Data_Engineering/main/Datasets/Wine.csv Put the Downloaded File in HDFS\na. Copy the File to cluster-master Container :\ndocker cp Wine.csv cluster-master:/ b. Access the cluster-master Container Shell :\ndocker exec -it cluster-master bash c.Create the Directory in HDFS\nhdfs dfs -mkdir -p /user/root/hdfs_odev d.Copy the File from Container to HDFS\nhdfs dfs -put Wine.csv /user/root/hdfs_odev/ e.Verify the File in HDFS :\nhdfs dfs -ls /user/root/hdfs_odev Copy the HDFS File to Another Directory\na.Create the Target Directory in HDFS\nhdfs dfs -mkdir -p /tmp/hdfs_odev b.Copy the File within HDFS :\nhdfs dfs -cp /user/root/hdfs_odev/Wine.csv /tmp/hdfs_odev/ c.Verify the Copy in the Target Directory :\nhdfs dfs -ls /tmp/hdfs_odev Delete the Directory with Skipping the Trash\na.Delete the Directory /tmp/hdfs_odev :\nhdfs dfs -rm -r -skipTrash /tmp/hdfs_odev b.Verify Deletion\nhdfs dfs -ls /tmp Explore the File in Namenode Web UI\nNavigate to Namenode Web UI : Open your browser and go to http://localhost:9870. Go to \u0026ldquo;Utilities -\u0026raquo; Browse the file system\u0026rdquo;. Navigate to /user/root/hdfs_odev/Wine.csv. Check File Details : Size : Size of the file. Replication Factor : Number of replicas. Block Size : Size of each block in HDFS. YARN Operations # Since the resource manager is running on cluster-master container, we first connect to its shell and initiate spark-shell in yarn mode to observe it as an application submitted.\nTo do this, first connect to the container shell:\ndocker exec -it cluster-master bash Then initiate pyspark session on yarn:\npyspark --master yarn The submitted PySparkShell application can now be observed from the YARN web UI:\nThis can also be queried from the container shell itself via below command:\nyarn application -list 2024-05-26 22:33:24,560 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at cluster-master/172.18.0.3:8032 Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):1 Application-Id\tApplication-Name\tApplication-Type\tUser\tQueue\tState\tFinal-State\tProgress\tTracking-URL application_1716762500232_0001\tPySparkShell\tSPARK\troot\tdefault\tRUNNING\tUNDEFINED\t10%\thttp://cluster-master:4040 You can query the status of this application\nyarn application -status application_1716762500232_0001 2024-05-26 22:34:05,252 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at cluster-master/172.18.0.3:8032 2024-05-26 22:34:10,228 INFO conf.Configuration: resource-types.xml not found 2024-05-26 22:34:10,238 INFO resource.ResourceUtils: Unable to find \u0026#39;resource-types.xml\u0026#39;. Application Report : Application-Id : application_1716762500232_0001 Application-Name : PySparkShell Application-Type : SPARK User : root Queue : default Application Priority : 0 Start-Time : 1716762642037 Finish-Time : 0 Progress : 10% State : RUNNING Final-State : UNDEFINED Tracking-URL : http://cluster-master:4040 RPC Port : -1 AM Host : 172.18.0.4 Aggregate Resource Allocation : 756961 MB-seconds, 469 vcore-seconds Aggregate Resource Preempted : 0 MB-seconds, 0 vcore-seconds Log Aggregation Status : DISABLED Diagnostics : Unmanaged Application : false Application Node Label Expression : `\u0026lt;Not set\u0026gt;` AM container Node Label Expression : \u0026lt;DEFAULT_PARTITION\u0026gt; TimeoutType : LIFETIME\tExpiryTime : UNLIMITED\tRemainingTime : -1seconds That\u0026rsquo;s all for this article. As a summary, we have setup a 3 node Hadoop Cluster on Docker environment and perform sample operations on HDFS and YARN.\nHope you find the article useful. For the next article, we will be performing operations on Hive and MapReduce. Stay tuned.\n","date":"2024-05-09","permalink":"/posts/20240509-hadoop-deploy/","section":"Posts","summary":"This article is about how to deploy Hadoop Cluster, which components it has, how the data is stored and managed in HDFS, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Hadoop Cluster Deployment and Operations"},{"content":"","date":"2024-05-09","permalink":"/tags/namenode/","section":"Tags","summary":"","title":"Namenode"},{"content":"","date":"2024-05-09","permalink":"/tags/yarn/","section":"Tags","summary":"","title":"Yarn"},{"content":"Build Docker Images with Multiarch Support # When working with the container images, you may come across an issue related to the mismatch of the platforms of the docker engine and the container image itself.\nThe requested image\u0026#39;s platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested I am running Colima as a docker engine on my Apple Mac with silicon processor which is an ARM64 based architecture (platform). When the image of the container I try to deploy on my Colima is built for an architecture other than ARM64 (ex: AMD64), it gives an incompatible platform error.\nOne way of recover from that error is to start Colima on Mac silicon with QEMU rosetta emulation of x86_64 arch type:\ncolima start --cpu 4 --memory 8 --arch x86_64 This way, we could be able to deploy AMD64 based docker images on our docker environments, however, since it is virtualizing the underlying processing architecture, there is a significant performance decrease occurs.\nTherefore, the best way is to re-build those images to make them multiarchitecture compatible, and run them on the Colima docker engine without virtualization.\nTo do this, follow the below steps:\nInstall Colima using homebrew if you have not already:\nbrew install colima Start colima which is compatible with the underlying silicon architecture. If you do not specify the architecture, Colima will use the underlying architecture of the machine anyway:\ncolima start --cpu 4 --memory 8 --arch aarch64 You can verify the colima is running by the command colima status.\nInstall Docker Buildx\na. Download buildx to your local, below is the latest version as of now:\nhttps://github.com/docker/buildx/releases/download/v0.13.1/buildx-v0.13.1.darwin-amd64\nb. Create plug-ins folder if you do not have any:\nmkdir -p ~/.docker/cli-plugins c. Move the downloaded Buildx binary to the ~/.docker/cli-plugins directory and rename it to docker-buildx via below command.\nmv buildx-v0.13.1.darwin-amd64 ~/.docker/cli-plugins/docker-buildx d. Make it executable:\nchmod +x ~/.docker/cli-plugins/docker-buildx e. Finally, verify the installation:\ndocker buildx version This should show the following output:\ngithub.com/docker/buildx v0.13.1 788433953af10f2a698f5c07611dddce2e08c7a0\nCreate a new Buildx builder instance with the name \u0026ldquo;multiplatform-builder\u0026rdquo;: docker buildx create --name multiplatform-builder\nUse the new builder instance by running: docker buildx use multiplatform-builder\nVerify that the builder instance is configured for multi-platform builds: docker buildx inspect --bootstrap\nName: multiplatform-builder Driver: docker-container Last Activity: 2024-04-14 12:03:28 +0000 UTC Nodes: Name: multiplatform-builder0 Endpoint: colima Status: running BuildKit daemon flags: --allow-insecure-entitlement=network.host BuildKit version: v0.13.1 Platforms: linux/arm64, linux/amd64, linux/amd64/v2 Now, let\u0026rsquo;s build a simple example Docker image for multiple platforms (e.g., linux/amd64 and linux/arm64):\n📝 Note: Replace the following \u0026ldquo;your-username\u0026rdquo; part with your Docker Hub username. The \u0026ndash;push flag is used to push the image to Docker Hub once the build is complete. Also modify the image-name and tag as you wish. The image to be built need to be represented with a dockerfile in the same directory where below command is executed. It is also a must to push the multi architecture images to a registry since the local repository does not support to store multi-architecture images\ndocker buildx build --platform linux/amd64,linux/arm64 -t \u0026lt;your-username\u0026gt;/\u0026lt;image-name\u0026gt;:\u0026lt;tag\u0026gt; . --push Finally, after the build completes, you can verify the multi-platform image on Docker Hub by visiting your image repository: https://hub.docker.com/r/your-username/multiplatform-image/tags\nYou should see the given tag with both \u0026ldquo;linux/amd64\u0026rdquo; and \u0026ldquo;linux/arm64\u0026rdquo; platforms listed.\n","date":"2024-04-21","permalink":"/posts/20240421-multiarch-build/","section":"Posts","summary":"This article is showing how to build any docker image for any architecture type, so that you can build your own architecture specific images and be able to run them on your docker engine.","title":"Build Docker Images with Multiarch Support"},{"content":"","date":"2024-04-21","permalink":"/categories/colima/","section":"Categories","summary":"","title":"Colima"},{"content":"","date":"2024-04-21","permalink":"/tags/colima/","section":"Tags","summary":"","title":"Colima"},{"content":"","date":"2024-04-21","permalink":"/tags/containers/","section":"Tags","summary":"","title":"Containers"},{"content":"","date":"2024-04-21","permalink":"/categories/microservices/","section":"Categories","summary":"","title":"Microservices"},{"content":"","date":"2024-04-21","permalink":"/tags/microservices/","section":"Tags","summary":"","title":"Microservices"},{"content":"","date":"2024-04-21","permalink":"/tags/multiarchitecture/","section":"Tags","summary":"","title":"Multiarchitecture"},{"content":"Experience Company Link Role Dates Location Ververica Technical Account Manager Present Oct 2023 Düsseldorf, DE Huawei Senior Solutions Architect Aug 2023 Nov 2022 Düsseldorf, DE Telefonica o2 Solution Delivery and Support Oct 2022 Dec 2016 Düsseldorf, DE Huawei Project Manager Nov 2016 Dec 2013 Istanbul, TR Engineering Team Lead Nov 2013 Oct 2010 Software Engineer Sep 2010 Jan 2010 Nortel Networks Technical Support Engineer Dec 2009 Jul 2008 \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Istanbul, TR Education School Link Degree Date Bahcesehir University Master of Business (MBA) 2009-2012 Ege University BSc, Computer Engineering 2004-2008 ","date":"2022-06-13","permalink":"/resume/","section":"Home Page","summary":"","title":"Resume"},{"content":"For over 15 years, I\u0026rsquo;ve been a part of the IT and Telecom world. My journey began as a Customer Support Engineer in Nortel Networks and has evolved through roles like Software Engineer, Engineering Team Lead, Project Manager, and Solutions Architect at Huawei. During this time, I\u0026rsquo;ve had the chance to work on impactful projects with companies such as Turkcell, Telefonica o2 and Deutsche Telekom.\nAt Huawei, I not only contributed to sales but also played a pivotal role in designing robust systems and ensuring efficient network solutions. I\u0026rsquo;ve been deeply involved with Huawei\u0026rsquo;s SmartCare® Solution, a leading Customer Experience Management product. This experience gave me hands-on expertise in onsite solution delivery, data integrations and management, from solutions like Hadoop and Spark to visualization platforms such as Tableau.\nCurrently I am working as Technical Account Manager at Ververica, specialized in open source realtime data streaming technologies around Apache Flink, both on-prem and cloud deployed mainly on Kubernetes.\nI have an academic degree of Computer Engineering from Ege University and MBA from Bahcesehir University. Beyond these, I hold PMP® and German B1 certifications.\nOutside the world of bits and bytes, I do travel with my wife and our lovely british shorthair \u0026ldquo;Bamboo\u0026rdquo; and take photos. Doing regular exercise, yoga and meditation also help me find balance.\nContact me Feel free to reach me from below platforms:\nLinkedIn Github e-mail ","date":"2022-06-13","permalink":"/about/","section":"Home Page","summary":"","title":"About"},{"content":" Hey there.\nThis is where I share what I find useful along the way via the occasional blog post.\nFeel free to look around, or click here to learn more about me.\n","date":"2022-06-13","permalink":"/","section":"Home Page","summary":"","title":"Home Page"},{"content":"","date":"2022-06-13","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"tbd\n","date":"0001-01-01","permalink":"/suggestions/","section":"Recs","summary":"","title":"Recs"},{"content":"","date":"0001-01-01","permalink":"/series/","section":"Series","summary":"","title":"Series"}]