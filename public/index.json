[{"content":"Airflow Introduction Pipeline # In this article, we will be deploying Apache Airflow, and create a sample pipeline which fetches data from a webserver and write into MinIO bucket.\n","date":"2024-06-09","permalink":"/posts/20240609-airflow-nginx-minio/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Airflow Introduction Pipeline"},{"content":"","date":"2024-06-09","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"2024-06-09","permalink":"/tags/catalog/","section":"Tags","summary":"","title":"Catalog"},{"content":"","date":"2024-06-09","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"Change Data Capture (CDC) Pipeline Implementation # In this article, we will be implementing a pipeline with PostgreSQL, Debezium CDC, Kafka, MinIO and the Spark.\n","date":"2024-06-09","permalink":"/posts/20240609-debezium-cdc-flink/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Change Data Capture (CDC) Pipeline Implementation"},{"content":"","date":"2024-06-09","permalink":"/categories/data-engineering/","section":"Categories","summary":"","title":"Data Engineering"},{"content":"","date":"2024-06-09","permalink":"/categories/docker/","section":"Categories","summary":"","title":"Docker"},{"content":"Elasticsearch Indexing and Kibana Dashboard with PySpark # In this article, we will be sinking data to ElasticSearch by PySpark and create a dashboard on Kibana\n","date":"2024-06-09","permalink":"/posts/20240609-elasticsearch-kibana/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Elasticsearch Indexing and Kibana Dashboard with PySpark"},{"content":"","date":"2024-06-09","permalink":"/categories/hadoop/","section":"Categories","summary":"","title":"Hadoop"},{"content":"","date":"2024-06-09","permalink":"/tags/hdfs/","section":"Tags","summary":"","title":"Hdfs"},{"content":"","date":"2024-06-09","permalink":"/tags/hive/","section":"Tags","summary":"","title":"Hive"},{"content":"","date":"2024-06-09","permalink":"/tags/mapreduce/","section":"Tags","summary":"","title":"Mapreduce"},{"content":" Technical Account Manager @ Ververica ","date":"2024-06-09","permalink":"/authors/nacisimsek/","section":"Authors","summary":"","title":"Naci Simsek"},{"content":"Optimizing Spark Applications # In this article, we will be deploying Hive services on Hadoop cluster\n","date":"2024-06-09","permalink":"/posts/20240609-spark-optimization/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Optimizing Spark Applications"},{"content":"","date":"2024-06-09","permalink":"/tags/postgres/","section":"Tags","summary":"","title":"Postgres"},{"content":"Processing Complex Nested JSON File with Spark # In this article, we will be processing complex nested JSON file with Apache Spark\n","date":"2024-06-09","permalink":"/posts/20240609-spark-json-process/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Processing Complex Nested JSON File with Spark"},{"content":"Spark Streaming Hands On from/to Kafka # In this article, we will be developing a Spark Streaming application which will read data from Kafka, process, and write back to Kafka\n","date":"2024-06-09","permalink":"/posts/20240609-spark-streaming/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Spark Streaming Hands On from/to Kafka"},{"content":"","date":"2024-06-09","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"2024-06-09","permalink":"/tags/tutorial/","section":"Tags","summary":"","title":"Tutorial"},{"content":"Submitting Spark Application # In this article, we will be submitting Spark application to the Spark cluster we previously deployed\n","date":"2024-06-08","permalink":"/posts/20240608-spark-submit/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Submitting Spark Application"},{"content":"Spark DataFrame Operations # In this article, we will be practicing Spark DataFrame operations\n","date":"2024-06-07","permalink":"/posts/20240607-spark-dataframe/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Spark DataFrame Operations"},{"content":"Use PySpark for Data Clean up # In this article, we will be cleaning up a dirty data by using PySpark\n","date":"2024-06-06","permalink":"/posts/20240606-spark-cleanup-data/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Use PySpark for Data Clean up"},{"content":"Deploy Spark Cluster # In this article, we will be deploying Spark Cluster on local, docker env, and Kubernetes\n","date":"2024-06-05","permalink":"/posts/20240605-spark-deploy/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Deploy Spark Cluster"},{"content":"","date":"2024-06-05","permalink":"/tags/spark/","section":"Tags","summary":"","title":"Spark"},{"content":"","date":"2024-06-05","permalink":"/categories/spark/","section":"Categories","summary":"","title":"Spark"},{"content":"Kafka Python Operations # In this article, we will be deploying Hive services on Hadoop cluster\n","date":"2024-06-04","permalink":"/posts/20240604-kafka-python-operations/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Kafka Python Operations"},{"content":"","date":"2024-06-03","permalink":"/tags/configuration/","section":"Tags","summary":"","title":"Configuration"},{"content":"","date":"2024-06-03","permalink":"/categories/deployment/","section":"Categories","summary":"","title":"Deployment"},{"content":"","date":"2024-06-03","permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker"},{"content":"","date":"2024-06-03","permalink":"/tags/kafka/","section":"Tags","summary":"","title":"Kafka"},{"content":"","date":"2024-06-03","permalink":"/categories/kafka/","section":"Categories","summary":"","title":"Kafka"},{"content":"Kafka Topics and Operations # This article is about how to operate on Kafka topics, their management, and configure important parameters\n","date":"2024-06-03","permalink":"/posts/20240603-kafka-topics/","section":"Posts","summary":"This article is about how to operate on Kafka topics, their management, and configure important parameters","title":"Kafka Topics and Operations"},{"content":"","date":"2024-06-03","permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes"},{"content":"","date":"2024-06-03","permalink":"/tags/topics/","section":"Tags","summary":"","title":"Topics"},{"content":"","date":"2024-06-02","permalink":"/tags/brew/","section":"Tags","summary":"","title":"Brew"},{"content":"Deploy Kafka Cluster # In this article, we will be deploying Kafka Cluster on local, docker env, and Kubernetes\n","date":"2024-06-02","permalink":"/posts/20240602-kafka-deploy/","section":"Posts","summary":"This article is about how to deploy Kafka Cluster on local (MacOS), docker and kubernetes","title":"Deploy Kafka Cluster"},{"content":"","date":"2024-06-02","permalink":"/tags/helm/","section":"Tags","summary":"","title":"Helm"},{"content":"Hive Deployment and Operations # In the previous article, we deployed our Hadoop cluster on docker, accessed both HDFS and YARN UIs, and performed some simple file operations on HDFS.\nIn this article, we will deploy Hive services on the same Hadoop cluster and perform various operations on it to learn its basics, usage, and advantages for our data operations. If you directly opened this article without setting up your Docker environment, I suggest you visit that article to deploy your cluster first.\n❗️ Important:\nSince the image used in this experiment is built with the needed Hive services included, we can run Hive services in this cluster. However, if you are working on your own cluster built on top of a different image, the operations/commands used in this article may not be compatible with your environment.\nIntroduction to Hive # Welcome to the world of Hive! If you\u0026rsquo;re new to this tool, don\u0026rsquo;t worry—by the end of this post, you\u0026rsquo;ll clearly understand what Hive is, why it was created, and how it works. Let\u0026rsquo;s get started!\nWhat is Hive? # Hive is like a friendly translator that helps you talk to your big data. Imagine you have a massive library filled with books (data), but they\u0026rsquo;re all written in different languages. You want to ask questions about the books but don\u0026rsquo;t speak those languages. Hive steps in to help! It allows you to use SQL (Structured Query Language), which is a common language for databases, to interact with your data stored in Hadoop, a framework for storing and processing large data sets.\nIt allows querying the data stored on HDFS with a SQL-like language. Created by Facebook. Handed over Apache community. It processes structured data that can be stored in a table. The mode in which it works is batch processing, not real-time (streaming data). It uses engines like MapReduce, Tez, or Spark. It supports many different file formats: Parquet, Sequence, ORC, Text, etc. ❗️ Important:\nHive is not a database. It is unsuitable for operational database needs (OLTP) since it focuses on heavy analytics queries. It is unsuitable for operational database needs (OLTP) for row-based insert, update, and delete or interactive queries. The query takes a reasonable amount of time. Converts query to MapReduce/Tez code gets resources from YARN and starts the operation. HiveQL is not standard SQL. Should not expect everything as in SQL. Why was Hive Built? # In the early days of Hadoop, data analysts and scientists had to write complex and lengthy MapReduce Java programs to analyze data. This was time-consuming, error-prone, and required a deep understanding of programming languages like Java or Python. Hive was built to simplify this process. It provides a familiar SQL-like interface that allows users to query and analyze data without writing complex code. This makes it easier for data analysts, scientists, and even business users to work with Big Data without requiring extensive programming knowledge.\nHow does Hive Work? # Hive works by converting your SQL queries into a series of MapReduce jobs that run on Hadoop. Let’s break this down with a simple example: Imagine you run a bookstore and have a massive spreadsheet (your data) that lists all your books, their authors, prices, and sales numbers. You want to discover which books by a specific author sold the most copies last year. With Hive, you can write a simple SQL query like:\nSELECT title, SUM(sales) FROM books WHERE author = \u0026#39;J.K. Rowling\u0026#39; AND year = 2022 GROUP BY title ORDER BY SUM(sales) DESC; When you run this query, Hive translates it into a series of steps that Hadoop understands (MapReduce jobs), processes the data, and then gives you the results. This way, you don’t need to worry about the complex underlying processes; you get the information you need quickly and efficiently. In summary, Hive acts as a bridge between you and your data, making it easier to ask questions and get answers without needing to dive into the complexities of Hadoop. It’s like having a knowledgeable assistant who can find the information you need and present it in a way that’s easy to understand.\nHive Architecture # The following are the major components of Apache Hive Architecture.\nMetastore (RDBMS): Think of the Metastore as a library catalog. It stores information about the data, like where it\u0026rsquo;s located, what it looks like, and how it\u0026rsquo;s organized. This helps Hive keep track of the data and make sense of it. In our cluster, it is PostgreSQL DB. Driver: The Driver is like a manager. It creates a session, receives instructions from the user, and monitors the execution process. It also keeps track of the metadata (information about the data) while running queries. Compiler: The Compiler is like a translator. It takes the HiveQL query (a question written in Hive\u0026rsquo;s language) and converts it into a plan that Hadoop MapReduce can understand. It breaks down the query into smaller steps and creates a roadmap for execution. Optimizer: The Optimizer is like a performance coach. It examines the plan created by the Compiler and finds ways to make it more efficient. For example, it might combine multiple steps into one or rearrange them to get the best results. Executor: The Executor is like a team leader. It takes the optimized plan and works with Hadoop to execute it. It ensures that all the necessary tasks are completed and in the right order. Thrift Server and CLI/UI: The Thrift Server is like a receptionist. External clients can interact with Hive using standard protocols like JDBC or ODBC. The CLI (Command-Line Interface) and UI (User Interface) allow users to run HiveQL queries and interact with Hive directly. Hive Operations # Without further theory, let’s dive into our hands-on exercises where we start hive services, deploy the metadata database and tables in the underlying RDBMS (it is PostgreSQL in our cluster), create Hive databases and tables, insert data into our tables, see how the data in these tables reflect in the underlying storage layer HDFS, learn the difference between internal and external table terms, play with partitioning and bucketing, and finally cover the performance optimization tips.\nStarting Hive Services # Check if the containers of our Hadoop cluster are up and running. See this chapter for instructions on deploying this cluster.\ndocker ps --format \u0026#39;table {{.ID}}\\t{{.Names}}\\t{{.Status}}\u0026#39; CONTAINER ID NAMES STATUS 6ad193be83c3 cluster-slave-1 Up 12 days d04cd3f43266 cluster-slave-2 Up 12 days fa725f0c0bd9 cluster-master Up 12 days 02571464b056 postgresql Up 12 days Logging into the shell of the container cluster-master\ndocker exec -it cluster-master bash Hive Schema Initialization # Initialize the Hive metastore schema in a PostgreSQL database\n❗️ Important:\nSchema initialization only needs to be performed for the first run of the Hive services. Once all the metadata tables are ready on Postgres, you need not initialize them again.\nschematool -initSchema -dbType postgres The output should look like this below.\nMetastore connection URL:\tjdbc:postgresql://postgresql:5432/metastore Metastore Connection Driver :\torg.postgresql.Driver Metastore connection User:\tpostgres Starting metastore schema initialization to 3.1.0 Initialization script hive-schema-3.1.0.postgres.sql Initialization script completed schemaTool completed We can verify on PostgreSQL that the database metastore is created with all its metadata tables:\nStarting Hive Services # We will then start the Hive services, which include:\nHive Metastore: A central repository that stores metadata about Hive tables, partitions, and schemas. HiveServer2: A service that allows clients to execute queries against Hive and retrieve results. $HADOOP_HOME/start-hive.sh Services starting up. Waiting for 60 seconds... Hive Metastore and HiveServer2 services have been started successfully. Connect Beeline HiveQL CLI # After the Hive service is started, we will connect to it using the Beeline CLI (Command Line Interface).\nThis command will connect us to a Hive server running on \u0026ldquo;cluster-master\u0026rdquo; using the default port 10000, allowing us to interact with Hive and run HiveQL queries.\nbeeline -u jdbc:hive2://cluster-master:10000 Connecting to jdbc:hive2://cluster-master:10000 Connected to: Apache Hive (version 3.1.3) Driver: Hive JDBC (version 2.3.9) Transaction isolation: TRANSACTION_REPEATABLE_READ Beeline version 2.3.9 by Apache Hive 📝 Note:\nIf you encounter the issue below when trying to connect to Beeline CLI, it’s most probably related to your Postgres container\u0026rsquo;s volume access.\nConnecting to jdbc:hive2://cluster-master:10000 Could not open connection to the HS2 server. Please check the server URI and if the URI is correct, then ask the administrator to check the server status. Error: Could not open client transport with JDBC Uri: jdbc:hive2://cluster-master:10000: java.net.ConnectException: Connection refused (Connection refused) (state=08S01,code=0) To fix it, either update the volume settings of the docker compose file and mount the container volumes to a local volume, or make sure to start containers with docker-compose up -d command executed by a root user or another user which has access to the local volume folders created by docker.\nConnect Hive2 JDBC Through SQL Editor # If you like to work on a SQL client rather than performing SQL queries on CLI, there is also an option to connect Hive via a SQL editor through JDBC. Here are the details to be used when connecting to Hive through a SQL Editor (ex: DBeaver):\nCreating a Hive Database and a Table # We are now ready to perform our HiveQL database and table operations on Beeline.\nList Databases and Tables # Below command is used to list the available databases:\nshow databases; The output will be shown as follows:\nINFO : Compiling command(queryId=root_20240818123321_a5886e81-31b2-493d-93e1-88e05b7431f7): show databases INFO : Concurrency mode is disabled, not creating a lock manager INFO : Semantic Analysis Completed (retrial = false) INFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:database_name, type:string, comment:from deserializer)], properties:null) INFO : Completed compiling command(queryId=root_20240818123321_a5886e81-31b2-493d-93e1-88e05b7431f7); Time taken: 0.016 seconds INFO : Concurrency mode is disabled, not creating a lock manager INFO : Executing command(queryId=root_20240818123321_a5886e81-31b2-493d-93e1-88e05b7431f7): show databases INFO : Starting task [Stage-0:DDL] in serial mode INFO : Completed executing command(queryId=root_20240818123321_a5886e81-31b2-493d-93e1-88e05b7431f7); Time taken: 0.017 seconds INFO : OK INFO : Concurrency mode is disabled, not creating a lock manager +----------------+ | database_name | +----------------+ | default | +----------------+ 1 row selected (0.119 seconds) 📝 Note:\nAs seen above, with the command output, also many other logs get pinted. Simply use the below command to turn the logging function off for this session:\nset hive.server2.logging.operation.level=NONE; If we would like to turn logging of system-wide, this needs to be set on the below config file of hive:\n./usr/local/hive/conf/hive-site.xml\nTo show the tables, simply use as below:\nshow tables; +-----------+ | tab_name | +-----------+ +-----------+ No rows selected (0.03 seconds) Loading Data into a Hive Table # We now have the environment ready to work with Hive databases and tables to create our own table and perform queries on it.\nTo perform our tests, we will be using CSV files to initiate data into our internal and external tables and see how Hive make use of data when the table is generated as internal table or external table.\nIn addition to creating internal and external tables based on a CSV dataset—which we’ll focus on in this chapter—there are several other methods to create and populate Hive tables:\nManually Creating a Table and Inserting Data with INSERT INTO Inserting Data from Another Table with CREATE TABLE AS SELECT Creating a Table Using SHOW CREATE TABLE Creating an Empty Table Based on Another Table with CREATE TABLE LIKE Creating Tables with Apache Spark For more information about the file formats Hive can read and write, you can check here.\nFor internal Hive tables, data itself and its metadata are both managed by Hive. If the internal table is dropped, the metadata and the table data that is kept in HDFS is deleted. However, for external tables, only the metadata of the table is managed by Hive, the data itself resides in its predefined HDFS location, even if the table is dropped.\nInternal vs. External Tables: Data Movement # Internal (Managed) Tables\nLOAD DATA INPATH: Action: Moves the data file from its current HDFS location to the table’s directory in the Hive warehouse. Result: The original data file is removed from its initial location after the move.\nLOAD DATA LOCAL INPATH: Action: Copies the data file from the local file system to the table’s directory in the Hive warehouse. Result: The original local data file remains intact.\nDropping the Table: Both metadata and data files are deleted from Hive’s warehouse directory.\nExternal Tables\nLOAD DATA INPATH: Action: Moves the data file from its current HDFS location to the external table’s specified directory. Result: The original data file is removed from its initial location after the move.\nLOAD DATA LOCAL INPATH: Action: Copies the data file from the local file system to the external table’s specified directory. Result: The original local data file remains intact.\nDropping the Table: Only the metadata is deleted. The data files remain at the external location in HDFS.\nInternal Hive Table Creation # Before proceding to create database and the respective table, first we download our dataset that we use to insert data into our Hive table. Because, before creating the table, we need to check our dataset, and collect some details about it as explained below, which the Hive table will use all these details to suit the table with that dataset.\nDownload Dataset to Container Local # Login to the container bash:\ndocker exec -it cluster-master bash Download the dataset. Make sure you download it to the folder where you map the docker volume (usr/local/hadoop/namenode/), if you would like to access the dataset even the container gets restarted.\nwget -O employee.txt https://raw.githubusercontent.com/nacisimsek/Data_Engineering/main/Datasets/employee.txt Here is how our data looks like:\ncat employee.txt name|work_place|gender_age|skills_score Michael|Montreal,Toronto|Male,30|DB:80,Network:88 Will|Montreal|Male,35|Perl:85,Scala:82 Shelley|New York|Female,27|Python:80,Spark:95 Lucy|Vancouver|Female,57|Sales:89,HR:94 Based on the content of the data here are the informations that we need to collect to be used when creating its Hive table:\nHas the dataset a header? Yes, 1 line What are each field names and their data types? name STRING work_place ARRAY\u0026lt;STRING\u0026gt; gender_age STRUCT\u0026lt;gender:STRING,age:INT\u0026gt; skills_score MAP\u0026lt;STRING,INT\u0026gt; How (with which character) each field is separated? With Pipe character \u0026#39;|\u0026#39; How each row (line) is separated? With a newline character \u0026#39;\\n\u0026#39; Does any column consists collection data type representing key:value pairs? Yes, collections in one column is separated by comma \u0026#39;,\u0026#39; Map keys are terminated by colon \u0026#39;:\u0026#39; Put the file into HDFS # We wiil place the dataset employee.txt into HDFS manually, then based on its HDFS directory, we will create its Hive table in the next step . In the directory of the container where we downloaded the dataset, execute below commands:\nAs we clarified earlier different options of loading data into the HDFS table, there are 2 different options:\nLoading data from HDFS: LOAD DATA INPATH Loading data from Local: LOAD DATA LOCAL INPATH We will be using the first option LOAD DATA INPATH since our dataset will be loaded to HDFS already manually, as follows:\nCreate the HDFS directory for our dataset:\nhdfs dfs -mkdir -p /user/datasets Put the file into this created HDFS directory:\nhdfs dfs -put employee.txt /user/datasets See the HDFS directory to verify that the dataset has successfully been placed there:\nhdfs dfs -ls /user/datasets Found 1 items -rw-r--r-- 1 root supergroup 215 2024-11-23 17:02 /user/datasets/employee.txt Create Hive Database and the Table # From Beeline (you can also use tools like DBeaver to connect Hive and execute HiveSQL queries), we will create a database called hive_db and a table called wine:\ncreate database if not exists hive_db; Show the existing databases:\nshow databases; |database_name| |-------------| |default | |hive_db | Describe particular database to see its details:\ndescribe database hive_db; |db_name|comment|location |owner_name|owner_type|parameters| |-------|-------|---------------------------------------------------------|----------|----------|----------| |hive_db| |hdfs://cluster-master:9000/user/hive/warehouse/hive_db.db|root |USER | | Select the created database:\nuse hive_db; Create the Hive table:\nWe will need to create a table that matches the structure of our dataset. Given the data and the data types we have identified earlier, the CREATE TABLE statement will look like this:\nCREATE TABLE employee ( name STRING, work_place ARRAY\u0026lt;STRING\u0026gt;, gender_age STRUCT\u0026lt;gender:STRING, age:INT\u0026gt;, skills_score MAP\u0026lt;STRING, INT\u0026gt; ) ROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;|\u0026#39; COLLECTION ITEMS TERMINATED BY \u0026#39;,\u0026#39; MAP KEYS TERMINATED BY \u0026#39;:\u0026#39; LINES TERMINATED BY \u0026#39;\\n\u0026#39; STORED AS TEXTFILE TBLPROPERTIES (\u0026#39;skip.header.line.count\u0026#39;=\u0026#39;1\u0026#39;); Explanation:\nname STRING: The name of the employee. work_place ARRAY \u0026lt;STRING\u0026gt;: A list of workplaces, since some entries have multiple locations separated by commas (e.g., Montreal,Toronto). gender_age STRUCT\u0026lt;gender:STRING, age:INT\u0026gt;: A structure containing the gender and age, separated by a comma (e.g., Male,30). skills_score MAP\u0026lt;STRING, INT\u0026gt;: A map of skills to scores, with skills and scores separated by colons and multiple entries separated by commas (e.g., DB:80,Network:88). Delimiters and Formatting:\nFIELDS TERMINATED BY \u0026lsquo;|\u0026rsquo;: Fields are separated by a pipe (|). COLLECTION ITEMS TERMINATED BY \u0026lsquo;,\u0026rsquo;: Items in arrays, structs, and maps are separated by commas. MAP KEYS TERMINATED BY \u0026lsquo;:\u0026rsquo;: In maps, keys and values are separated by colons. LINES TERMINATED BY \u0026lsquo;\\n\u0026rsquo;: Each record is on a new line. STORED AS TEXTFILE: Specifies that the data is stored in a text file format. TBLPROPERTIES (\u0026lsquo;skip.header.line.count\u0026rsquo;=\u0026lsquo;1\u0026rsquo;): Skips the header line in your data file. Note: The TBLPROPERTIES (\u0026lsquo;skip.header.line.count\u0026rsquo;=\u0026lsquo;1\u0026rsquo;) property is essential here because your dataset includes a header row that you don’t want to import as data.\nLoad Data into Table # Since our data file employee.txt is already in HDFS at /user/datasets/employee.txt, we can load it into our internal table using the LOAD DATA INPATH command:\nLOAD DATA INPATH \u0026#39;/user/datasets/employee.txt\u0026#39; INTO TABLE employee; ❗️ Important:\nData Movement : This command will move the employee.txt file from its current HDFS location into the Hive table’s directory within the Hive warehouse. After this operation, the original file at /user/datasets/employee.txt will no longer exist.\nInternal Table : Since we are creating an internal (managed) table, Hive assumes responsibility for the data files. Dropping the table later will delete both the table metadata and the data files.\nVerify That the Data Is Loaded Correctly # We can run a simple SELECT query to check that your data has been loaded properly:\nSELECT * FROM employee; +----------------+-------------------------+-------------------------------+---------------------------+ | employee.name | employee.work_place | employee.gender_age | employee.skills_score | +----------------+-------------------------+-------------------------------+---------------------------+ | Michael | [\u0026#34;Montreal\u0026#34;,\u0026#34;Toronto\u0026#34;] | {\u0026#34;gender\u0026#34;:\u0026#34;Male\u0026#34;,\u0026#34;age\u0026#34;:30} | {\u0026#34;DB\u0026#34;:80,\u0026#34;Network\u0026#34;:88} | | Will | [\u0026#34;Montreal\u0026#34;] | {\u0026#34;gender\u0026#34;:\u0026#34;Male\u0026#34;,\u0026#34;age\u0026#34;:35} | {\u0026#34;Perl\u0026#34;:85,\u0026#34;Scala\u0026#34;:82} | | Shelley | [\u0026#34;New York\u0026#34;] | {\u0026#34;gender\u0026#34;:\u0026#34;Female\u0026#34;,\u0026#34;age\u0026#34;:27} | {\u0026#34;Python\u0026#34;:80,\u0026#34;Spark\u0026#34;:95} | | Lucy | [\u0026#34;Vancouver\u0026#34;] | {\u0026#34;gender\u0026#34;:\u0026#34;Female\u0026#34;,\u0026#34;age\u0026#34;:57} | {\u0026#34;Sales\u0026#34;:89,\u0026#34;HR\u0026#34;:94} | +----------------+-------------------------+-------------------------------+---------------------------+ We can perform queries that access nested fields within our complex data types.\nTo select individual fields from the gender_age struct:\nSELECT name, gender_age.gender AS gender, gender_age.age AS age FROM employee; +----------+---------+------+ | name | gender | age | +----------+---------+------+ | Michael | Male | 30 | | Will | Male | 35 | | Shelley | Female | 27 | | Lucy | Female | 57 | +----------+---------+------+ To get all workplaces or a specific workplace:\nAll workspaces:\nSELECT name, work_place FROM employee; +----------+-------------------------+ | name | work_place | +----------+-------------------------+ | Michael | [\u0026#34;Montreal\u0026#34;,\u0026#34;Toronto\u0026#34;] | | Will | [\u0026#34;Montreal\u0026#34;] | | Shelley | [\u0026#34;New York\u0026#34;] | | Lucy | [\u0026#34;Vancouver\u0026#34;] | +----------+-------------------------+ First workplace:\nSELECT name, work_place[0] AS primary_work_place FROM employee; +----------+---------------------+ | name | primary_work_place | +----------+---------------------+ | Michael | Montreal | | Will | Montreal | | Shelley | New York | | Lucy | Vancouver | +----------+---------------------+ Find the employees that Python skill is greter than 70.\nSELECT name, skills_score[\u0026#39;Python\u0026#39;] as Python FROM employee WHERE skills_score[\u0026#39;Python\u0026#39;] \u0026gt; 70; +----------+---------+ | name | python | +----------+---------+ | Shelley | 80 | +----------+---------+ Drop Database and its Table # External Hive Table Creation # File Formats and Compressions # Partitioning and Bucketing # ","date":"2024-06-01","permalink":"/posts/20240601-hive/","section":"Posts","summary":"This article is about how to deploy Hive services on Hadoop Cluster, which components it has, how the data is stored and managed in Hive, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Hive Setup and Operations"},{"content":"","date":"2024-05-09","permalink":"/tags/datanode/","section":"Tags","summary":"","title":"Datanode"},{"content":"Hadoop Cluster Deployment and Operations # In this article, we will be deploying Hadoop cluster on our local Docker environment with 1 Master (namenode \u0026amp; datanode) and 2 slave nodes (datanodes only) and perform data operations. We will be storing data into HDFS, and observe its operations on Hadoop UI.\nIn the world of big data, managing and processing vast amounts of information efficiently has always been a challenge. Traditional systems struggled to keep up with the growing volume, velocity, and variety of data. This is where Hadoop, an open-source framework, revolutionized the landscape of data processing and storage.\nIntroduction to Hadoop # What is Hadoop? # Hadoop is designed to store and process large datasets across clusters of computers using a simple programming model. It is highly scalable, cost-effective, and fault-tolerant, making it ideal for handling big data.\nWhy was Hadoop Built? # Before Hadoop, traditional systems struggled with the limitations of single-node processing and storage, leading to issues like high costs, limited scalability, and poor fault tolerance. Hadoop was designed to address these challenges by:\nScalability : Easily adding more nodes to handle increasing data loads. Cost-Effectiveness : Using commodity hardware to build large clusters. Fault Tolerance : Ensuring data availability and reliability even if some nodes fail. High Throughput : Efficiently processing large datasets in parallel. Comparing to Legacy Systems\nTraditional data processing systems were often limited by their ability to scale and handle large, diverse datasets efficiently. They typically relied on expensive, high-end hardware and faced significant challenges in terms of fault tolerance and scalability. Hadoop, with its distributed architecture, addressed these limitations by providing a robust, scalable, and cost-effective solution for big data processing.\nWith Hadoop, data storage and the computation both handled on the nodes which consist the Hadoop cluster.\nHow does Hadoop Work? # Hadoop\u0026rsquo;s architecture is built around three main components: HDFS, MapReduce, and YARN.\nHDFS (Hadoop Distributed File System) : # Purpose : HDFS is designed to store large files by distributing them across multiple machines in a cluster.\nHow It Works : HDFS breaks down a large file into smaller blocks and stores them across different nodes in the cluster. This distribution allows for parallel processing and ensures data availability, even if some nodes fail.\nLogic : By spreading the data, HDFS provides high throughput and reliability, addressing the limitations of single-node storage systems.\nModules:\nNameNode: Manages HDFS metadata and namespace. These nodes does not store data, but they actually keep the metadata of the data that is kept in data nodes, such as which data nodes the data is splitted, what is its replication, etc\u0026hellip;\nDataNode: Stores actual HDFS data blocks. These are the nodes where the actual data blocks are kept. When user would like to read/write data from/to HDFS, after getting the metadata information from NameNode, client is communicating these nodes for data operations.\nSecondary/Standby NameNode: Periodically saves the merged namespace image to reduce NameNode load and also grant High Availability (HA) for the cluster.\nBelow is a video by Jesse Anderson where he is explaining how the data is kept as blocks in HDFS.\nMapReduce : # Purpose : MapReduce is the core processing engine of Hadoop, designed to process large datasets in parallel.\nHow It Works : It breaks down a task into two main functions: Map and Reduce.\nMap Function : Processes input data and converts it into a set of intermediate key-value pairs.\nReduce Function : Merges these intermediate values to produce the final output.\nLogic : This parallel processing model allows Hadoop to handle large-scale data analysis efficiently, overcoming the bottlenecks of traditional sequential processing.\nYARN (Yet Another Resource Negotiator) : # Purpose : YARN manages and allocates resources to various applications running in a Hadoop cluster. How It Works : It consists of a ResourceManager and NodeManagers. The ResourceManager allocates resources based on the needs of the applications, while NodeManagers monitor resources on individual nodes. Logic : YARN enhances Hadoop’s scalability and resource utilization, enabling multiple data processing engines to run simultaneously on a single cluster. Modules: ResourceManager: Manages resource allocation in the YARN ecosystem.\nNodeManager: Manages containers and resources on individual nodes in YARN.\nApplicationMaster: Manages the execution (scheduling and coordination) of a single application in YARN during the application lifecycle and got removed as soon as the application terminates.\nBelow is the representation of the job submission and its management on YARN\nWhat are the Disadvantages of Hadoop Comparing to Modern Data Systems? # Hadoop was a groundbreaking solution for big data, but modern data systems have emerged with enhancements that address many of its limitations. Here’s a look at some key disadvantages of Hadoop:\nComplexity in Management and Configuration: Requires specialized knowledge for setup, configuration, and maintenance, which can be complex and time-consuming. Performance and Latency: Primarily batch-oriented with high latency in processing large datasets. Resource Efficiency: Often resource-intensive with significant computational and storage demands. Flexibility and Ecosystem Integration: Limited flexibility with a strong focus on MapReduce and a more rigid ecosystem. Scalability and Elasticity: Adding new nodes is not fast or easy, requiring manual intervention and planning. Data Handling and Processing Capabilities: Computation and storage are tightly coupled, limiting flexibility in resource allocation. Deployment of the Cluster # We will deploy the cluster by using the following docker file:\nhttps://raw.githubusercontent.com/nacisimsek/Data_Engineering/main/Hadoop/docker-compose.yaml\n📝 Note:\nThe image which is being used in this docker-compose file is my multiarch built version of the image which was originally prepared by Veribilimiokulu. I had attended their data engineering bootcamp program and had a chance to learn many new skills around data engineering while also refreshing my existing knowledge. Many of the next blog posts in my website will be related to the hands-on experience I gained during this bootcamp, therefore, special thanks to them for helping us improve ourselves and also encouraging us to share our knowledge to others.\nSimply copy the docker compose file and execute below command to deploy the containers.\ndocker-compose up -d This will compose the following four containers:\ncluster-master cluster-slave-1 cluster-slave-2 postgresql List the containers and their status with the following command:\ndocker ps --format \u0026#39;table {{.ID}}\\t{{.Names}}\\t{{.Status}}\u0026#39; CONTAINER ID NAMES STATUS 362d93c0d28a cluster-slave-1 Up About an hour 5e69cc3072aa cluster-slave-2 Up About an hour bd3276aa0e7f cluster-master Up About an hour 63ea237d5907 postgresql Up About an hour After the containers are started, make sure each container has started the above mentioned HDFS and YARN specific modules successfully.\nTo check this, need to connect the shell of each container:\ndocker exec -it cluster-master bash Then perform below command to see the started modules (services):\nroot@cluster-master:/# jps 455 NameNode 637 Jps 110 GetConf Since this is our master node of YARN and HDFS, and also will be used as one of our data nodes in HDFS and a worker node for YARN, we need to make sure all below modules (services) to be running on it:\nResourceManager (YARN) NodeManager (YARN) DataNode (HDFS) Execute below commands to start these services if they have not been started:\nTo start NodeManager and ResourceManager:\n/usr/local/hadoop/sbin/start-yarn.sh To start DataNode:\n/usr/local/hadoop/sbin/hadoop-daemon.sh start datanode Finally, check if all modules have been started successfully:\nroot@cluster-master:/# jps 903 ResourceManager 455 NameNode 1815 Jps 1560 DataNode 1163 NodeManager 110 GetConf And this is how the slave nodes should look like:\nroot@cluster-slave-1:/# jps 496 DataNode 2017 Jps 1618 NodeManager 837 SecondaryNameNode We should be now accessing to the Hadoop NameNode Web UI (Port 9870) and YARN ResourceManager Web UI (Port 8088)\nPort 9870: Hadoop NameNode Web UI # You can access the namenode web UI from your browser: http://localhost:9870/\nPurpose : The web interface on port 9870 is the Hadoop NameNode Web UI. It is used for monitoring the HDFS (Hadoop Distributed File System). Functions : View HDFS Health : Provides an overview of the HDFS, including the health and status of the NameNode. Browse File System : Allows users to browse the HDFS directories and files. Check DataNode Status : Displays the status and details of all DataNodes in the cluster, including storage utilization and block distribution. Monitor Replication : Shows information about block replication and under-replicated blocks. View Logs : Access NameNode logs for troubleshooting and monitoring. Key Features : HDFS Overview : Presents a summary of the total and available storage. DataNodes Information : Details on each DataNode’s storage capacity, usage, and health. HDFS Metrics : Metrics on file system operations, such as read and write requests. 📝 Note:\nIf you do not see all three nodes listed as Datanode in above list, its most likely the DataNode service is stopped or should be restarted on those nodes. If so, you can connect to the respective container\u0026rsquo;s shell and restart DataNode service as follows:\ndocker exec -it \u0026lt;container_name\u0026gt; /bin/bash hdfs --daemon start datanode ❗️ Important:\nNormally in commercial systems, the master node should not be using as a DataNode, but here in this cluster, for testing purposes, we deployed the master node is also one of the DataNode.\nPort 8088: YARN ResourceManager Web UI # You can access the YARN resource manager web UI from your browser: http://localhost:8088/\nPurpose : The web interface on port 8088 is the YARN ResourceManager Web UI. It is used for managing and monitoring YARN (Yet Another Resource Negotiator), which handles resource allocation and job scheduling in the Hadoop cluster. Functions : Monitor Applications : Displays the status of running and completed applications (jobs) within the cluster. View Cluster Metrics : Provides metrics on resource usage, including memory and CPU utilization across the cluster. Track Application Logs : Allows users to access logs for individual applications, aiding in troubleshooting and performance analysis. Manage Nodes : Lists all the nodes in the cluster with details about their resource usage and health. Key Features : Application Overview : Summarizes the state, resource usage, and history of applications. Cluster Utilization : Shows real-time data on how resources are being utilized across the cluster. Node Management : Information on each NodeManager, including available and used resources. 📝 Note:\nIf you do not see all three nodes listed as Active Nodes in above page, its most likely the NodeManager service is stopped or should be restarted on those nodes. If so, you can connect to the respective container\u0026rsquo;s shell and restartNodeManager service as follows:\n\u0026gt; docker exec -it cluster-slave-2 /bin/bash root@cluster-slave-2:/# jps 480 DataNode 929 GetConf 1416 Jps 798 SecondaryNameNode /usr/local/hadoop/sbin/yarn-daemon.sh start nodemanager Cluster Operations # We will be performing operations on HDFS and YARN to get familiar with them.\nHDFS Operations # Download the CSV File to Local, which we will use to import to HDFS: wget https://raw.githubusercontent.com/nacisimsek/Data_Engineering/main/Datasets/Wine.csv Put the Downloaded File in HDFS\na. Copy the File to cluster-master Container :\ndocker cp Wine.csv cluster-master:/ b. Access the cluster-master Container Shell :\ndocker exec -it cluster-master bash c.Create the Directory in HDFS\nhdfs dfs -mkdir -p /user/root/hdfs_odev d.Copy the File from Container to HDFS\nhdfs dfs -put Wine.csv /user/root/hdfs_odev/ e.Verify the File in HDFS :\nhdfs dfs -ls /user/root/hdfs_odev Copy the HDFS File to Another Directory\na.Create the Target Directory in HDFS\nhdfs dfs -mkdir -p /tmp/hdfs_odev b.Copy the File within HDFS :\nhdfs dfs -cp /user/root/hdfs_odev/Wine.csv /tmp/hdfs_odev/ c.Verify the Copy in the Target Directory :\nhdfs dfs -ls /tmp/hdfs_odev Delete the Directory with Skipping the Trash\na.Delete the Directory /tmp/hdfs_odev :\nhdfs dfs -rm -r -skipTrash /tmp/hdfs_odev b.Verify Deletion\nhdfs dfs -ls /tmp Explore the File in Namenode Web UI\nNavigate to Namenode Web UI : Open your browser and go to http://localhost:9870. Go to \u0026ldquo;Utilities -\u0026raquo; Browse the file system\u0026rdquo;. Navigate to /user/root/hdfs_odev/Wine.csv. Check File Details : Size : Size of the file. Replication Factor : Number of replicas. Block Size : Size of each block in HDFS. YARN Operations # Since the resource manager is running on cluster-master container, we first connect to its shell and initiate spark-shell in yarn mode to observe it as an application submitted.\nTo do this, first connect to the container shell:\ndocker exec -it cluster-master bash Then initiate pyspark session on yarn:\npyspark --master yarn The submitted PySparkShell application can now be observed from the YARN web UI:\nThis can also be queried from the container shell itself via below command:\nyarn application -list 2024-05-26 22:33:24,560 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at cluster-master/172.18.0.3:8032 Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):1 Application-Id\tApplication-Name\tApplication-Type\tUser\tQueue\tState\tFinal-State\tProgress\tTracking-URL application_1716762500232_0001\tPySparkShell\tSPARK\troot\tdefault\tRUNNING\tUNDEFINED\t10%\thttp://cluster-master:4040 You can query the status of this application\nyarn application -status application_1716762500232_0001 2024-05-26 22:34:05,252 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at cluster-master/172.18.0.3:8032 2024-05-26 22:34:10,228 INFO conf.Configuration: resource-types.xml not found 2024-05-26 22:34:10,238 INFO resource.ResourceUtils: Unable to find \u0026#39;resource-types.xml\u0026#39;. Application Report : Application-Id : application_1716762500232_0001 Application-Name : PySparkShell Application-Type : SPARK User : root Queue : default Application Priority : 0 Start-Time : 1716762642037 Finish-Time : 0 Progress : 10% State : RUNNING Final-State : UNDEFINED Tracking-URL : http://cluster-master:4040 RPC Port : -1 AM Host : 172.18.0.4 Aggregate Resource Allocation : 756961 MB-seconds, 469 vcore-seconds Aggregate Resource Preempted : 0 MB-seconds, 0 vcore-seconds Log Aggregation Status : DISABLED Diagnostics : Unmanaged Application : false Application Node Label Expression : \u0026lt;Not set\u0026gt; AM container Node Label Expression : \u0026lt;DEFAULT_PARTITION\u0026gt; TimeoutType : LIFETIME\tExpiryTime : UNLIMITED\tRemainingTime : -1seconds That\u0026rsquo;s all for this article. As a summary, we have setup a 3 node Hadoop Cluster on Docker environment and perform sample operations on HDFS and YARN.\nHope you find the article useful. For the next article, we will be performing operations on Hive and MapReduce. Stay tuned.\n","date":"2024-05-09","permalink":"/posts/20240509-hadoop-deploy/","section":"Posts","summary":"This article is about how to deploy Hadoop Cluster, which components it has, how the data is stored and managed in HDFS, how the calculation is done via MapReduce, and how Yarn manage the resources","title":"Hadoop Cluster Deployment and Operations"},{"content":"","date":"2024-05-09","permalink":"/tags/namenode/","section":"Tags","summary":"","title":"Namenode"},{"content":"","date":"2024-05-09","permalink":"/tags/yarn/","section":"Tags","summary":"","title":"Yarn"},{"content":"Build Docker Images with Multiarch Support # When working with the container images, you may come across an issue related to the mismatch of the platforms of the docker engine and the container image itself.\nThe requested image\u0026#39;s platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested I am running Colima as a docker engine on my Apple Mac with silicon processor which is an ARM64 based architecture (platform). When the image of the container I try to deploy on my Colima is built for an architecture other than ARM64 (ex: AMD64), it gives an incompatible platform error.\nOne way of recover from that error is to start Colima on Mac silicon with QEMU rosetta emulation of x86_64 arch type:\ncolima start --cpu 4 --memory 8 --arch x86_64 This way, we could be able to deploy AMD64 based docker images on our docker environments, however, since it is virtualizing the underlying processing architecture, there is a significant performance decrease occurs.\nTherefore, the best way is to re-build those images to make them multiarchitecture compatible, and run them on the Colima docker engine without virtualization.\nTo do this, follow the below steps:\nInstall Colima using homebrew if you have not already:\nbrew install colima Start colima which is compatible with the underlying silicon architecture. If you do not specify the architecture, Colima will use the underlying architecture of the machine anyway:\ncolima start --cpu 4 --memory 8 --arch aarch64 You can verify the colima is running by the command colima status.\nInstall Docker Buildx\na. Download buildx to your local, below is the latest version as of now:\nhttps://github.com/docker/buildx/releases/download/v0.13.1/buildx-v0.13.1.darwin-amd64\nb. Create plug-ins folder if you do not have any:\nmkdir -p ~/.docker/cli-plugins c. Move the downloaded Buildx binary to the ~/.docker/cli-plugins directory and rename it to docker-buildx via below command.\nmv buildx-v0.13.1.darwin-amd64 ~/.docker/cli-plugins/docker-buildx d. Make it executable:\nchmod +x ~/.docker/cli-plugins/docker-buildx e. Finally, verify the installation:\ndocker buildx version This should show the following output:\ngithub.com/docker/buildx v0.13.1 788433953af10f2a698f5c07611dddce2e08c7a0\nCreate a new Buildx builder instance with the name \u0026ldquo;multiplatform-builder\u0026rdquo;: docker buildx create --name multiplatform-builder\nUse the new builder instance by running: docker buildx use multiplatform-builder\nVerify that the builder instance is configured for multi-platform builds: docker buildx inspect --bootstrap\nName: multiplatform-builder Driver: docker-container Last Activity: 2024-04-14 12:03:28 +0000 UTC Nodes: Name: multiplatform-builder0 Endpoint: colima Status: running BuildKit daemon flags: --allow-insecure-entitlement=network.host BuildKit version: v0.13.1 Platforms: linux/arm64, linux/amd64, linux/amd64/v2 Now, let\u0026rsquo;s build a simple example Docker image for multiple platforms (e.g., linux/amd64 and linux/arm64):\n📝 Note: Replace the following \u0026ldquo;your-username\u0026rdquo; part with your Docker Hub username. The \u0026ndash;push flag is used to push the image to Docker Hub once the build is complete. Also modify the image-name and tag as you wish. The image to be built need to be represented with a dockerfile in the same directory where below command is executed. It is also a must to push the multi architecture images to a registry since the local repository does not support to store multi-architecture images\ndocker buildx build --platform linux/amd64,linux/arm64 -t \u0026lt;your-username\u0026gt;/\u0026lt;image-name\u0026gt;:\u0026lt;tag\u0026gt; . --push Finally, after the build completes, you can verify the multi-platform image on Docker Hub by visiting your image repository: https://hub.docker.com/r/your-username/multiplatform-image/tags\nYou should see the given tag with both \u0026ldquo;linux/amd64\u0026rdquo; and \u0026ldquo;linux/arm64\u0026rdquo; platforms listed.\n","date":"2024-04-21","permalink":"/posts/20240421-multiarch-build/","section":"Posts","summary":"This article is showing how to build any docker image for any architecture type, so that you can build your own architecture specific images and be able to run them on your docker engine.","title":"Build Docker Images with Multiarch Support"},{"content":"","date":"2024-04-21","permalink":"/tags/colima/","section":"Tags","summary":"","title":"Colima"},{"content":"","date":"2024-04-21","permalink":"/categories/colima/","section":"Categories","summary":"","title":"Colima"},{"content":"","date":"2024-04-21","permalink":"/tags/containers/","section":"Tags","summary":"","title":"Containers"},{"content":"","date":"2024-04-21","permalink":"/tags/microservices/","section":"Tags","summary":"","title":"Microservices"},{"content":"","date":"2024-04-21","permalink":"/categories/microservices/","section":"Categories","summary":"","title":"Microservices"},{"content":"","date":"2024-04-21","permalink":"/tags/multiarchitecture/","section":"Tags","summary":"","title":"Multiarchitecture"},{"content":"Books # Design of Everyday Things # asdasd\nDesign of Everyday Things # asdasd\nDesign of Everyday Things # asdasd\nZero to One # asdasd\nLean Startup # asdasd\nZero to One # asdasd\nArticles # blabla # asdasd\nOnline Publications # Stratechery # asdasd\nHBR # asdasd\nPodcasts # bla bal # ","date":"2023-11-16","permalink":"/suggestions/no-rules-rules-copy-2/","section":"Recs","summary":"tbd","title":"No Rules Rules"},{"content":"Books # Design of Everyday Things # asdasd\nDesign of Everyday Things # asdasd\nDesign of Everyday Things # asdasd\nZero to One # asdasd\nLean Startup # asdasd\nZero to One # asdasd\nArticles # blabla # asdasd\nOnline Publications # Stratechery # asdasd\nHBR # asdasd\nPodcasts # bla bal # ","date":"2023-11-16","permalink":"/suggestions/no-rules-rules-copy-3/","section":"Recs","summary":"tbd","title":"No Rules Rules"},{"content":"Books # Design of Everyday Things # asdasd\nDesign of Everyday Things # asdasd\nDesign of Everyday Things # asdasd\nZero to One # asdasd\nLean Startup # asdasd\nZero to One # asdasd\nArticles # blabla # asdasd\nOnline Publications # Stratechery # asdasd\nHBR # asdasd\nPodcasts # bla bal # ","date":"2023-11-16","permalink":"/suggestions/no-rules-rules-copy/","section":"Recs","summary":"tbd","title":"No Rules Rules"},{"content":"Books # Design of Everyday Things # asdasd\nDesign of Everyday Things # asdasd\nDesign of Everyday Things # asdasd\nZero to One # asdasd\nLean Startup # asdasd\nZero to One # asdasd\nArticles # blabla # asdasd\nOnline Publications # Stratechery # asdasd\nHBR # asdasd\nPodcasts # bla bal # ","date":"2023-11-16","permalink":"/suggestions/no-rules-rules/","section":"Recs","summary":"tbd","title":"No Rules Rules"},{"content":"tbd\n","date":"2023-11-16","permalink":"/suggestions/","section":"Recs","summary":"","title":"Recs"},{"content":"test 1\nLorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas\n","date":"2022-06-19","permalink":"/mentor/","section":"Home Page","summary":"","title":"Mentor"},{"content":"Experience Company Link Role Dates Location Ververica Technical Account Manager Present Oct 2023 Düsseldorf, DE Huawei Senior Solutions Architect Aug 2023 Nov 2022 Düsseldorf, DE Telefonica o2 Solution Delivery and Support Oct 2022 Dec 2016 Düsseldorf, DE Huawei Project Manager Nov 2016 Dec 2013 Istanbul, TR Engineering Team Lead Nov 2013 Oct 2010 Software Engineer Sep 2010 Jan 2010 Nortel Networks Technical Support Engineer Dec 2009 Jul 2008 \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Istanbul, TR Education School Link Degree Date Bahcesehir University Master of Business (MBA) 2009-2012 Ege University BSc, Computer Engineering 2004-2008 ","date":"2022-06-13","permalink":"/resume/","section":"Home Page","summary":"","title":"Resume"},{"content":"For over 15 years, I\u0026rsquo;ve been a part of the IT and Telecom world. My journey began as a Customer Support Engineer in Nortel Networks and has evolved through roles like Software Engineer, Engineering Team Lead, Project Manager, and Solutions Architect at Huawei. During this time, I\u0026rsquo;ve had the chance to work on impactful projects with companies such as Turkcell, Telefonica o2 and Deutsche Telekom.\nAt Huawei, I not only contributed to sales but also played a pivotal role in designing robust systems and ensuring efficient network solutions. I\u0026rsquo;ve been deeply involved with Huawei\u0026rsquo;s SmartCare® Solution, a leading Customer Experience Management product. This experience gave me hands-on expertise in onsite solution delivery, data integrations and management, from solutions like Hadoop and Spark to visualization platforms such as Tableau.\nCurrently I am working as Technical Account Manager at Ververica, specialized in open source realtime data streaming technologies around Apache Flink, both on-prem and cloud deployed mainly on Kubernetes.\nI have an academic degree of Computer Engineering from Ege University and MBA from Bahcesehir University. Beyond these, I hold PMP® and German B1 certifications.\nOutside the world of bits and bytes, I do travel with my wife and our lovely british shorthair \u0026ldquo;Bamboo\u0026rdquo; and take photos. Doing regular exercise, yoga and meditation also help me find balance.\nContact me Feel free to reach me from below platforms:\nLinkedIn Github e-mail ","date":"2022-06-13","permalink":"/about/","section":"Home Page","summary":"","title":"About"},{"content":" Hey there.\nThis is where I share what I find useful along the way via the occasional blog post.\nFeel free to look around, or click here to learn more about me.\n","date":"2022-06-13","permalink":"/","section":"Home Page","summary":"","title":"Home Page"},{"content":"","date":"2022-06-13","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"I always try to find time to work and learn something new. Usually, most of these pet-projects don\u0026rsquo;t see the light of day. They are, however, great opportunities to try something in the real world and learn from it.\nLogo Title Description Link Blowfish A powerful, lightweight theme for Hugo built with Tailwind CSS. sitegithub Blowfish-Tools CLI to initialize a Blowfish project sitegithubNPM Wormhole A wormhole into the universe - web feed for deep space photography sitegithub ","date":"2022-06-13","permalink":"/projects/","section":"Home Page","summary":"","title":"Projects"},{"content":" Technical Account Manager @ Ververica ","date":"0001-01-01","permalink":"/authors/nunocoracao/","section":"Authors","summary":"","title":"Naci Simsek"},{"content":"","date":"0001-01-01","permalink":"/series/","section":"Series","summary":"","title":"Series"}]